[["index.html", "1 Marine Genomics Semester edition 1.1 Cloud computing versus individual laptops 1.2 Assessment 1.3 Short recorded demonstrations 1.4 Final demonstration", " 1 Marine Genomics Semester edition Welcome to Marine Genomics! This is a course on learning how to analyze genomic data for undergraduate and graduate students. We focused on marine organisms but these lessons can easily be applied to any taxa. We required that students have a background in biology and have completed the general biology series. Additionally, it is helpful if students have taken a course in Evolution, though this was not a requirement. We did not require any coding experience in R or Bash/UNIX. For most weeks, our course consists of one 75 min lecture and one 75 minute coding session per week. The lectures begin with a power point presentation where we introduce the weeks concept (for example, PCAs) and then are followed by a live coding demonstration carrier out by the instructor. The coding sessions provide an opportunity for the students to either repeat the same coding demonstration carried out during the lecture, or to extend the demonstration via coding exercises which we have written for each week and for which the solutions are available on our website (under each exercise). The first 5 weeks are primarily training in bash/UNIX and R basics and are structured as 2 lectures a week that are broken up with exercises for the students to practice each skill they are shown in live-coding demonstrations. The exercises typically take 5-15 minutes to complete. The remaining weeks are focused on performing a specific analysis (PCA, GWAS, Fst outlier analyses, etc). We provide all the data for the students to complete each weeks coding activities along with instructions for how to download the data. Additionally, the students found the website a phenomenally helpful resource. 1.1 Cloud computing versus individual laptops This course was designed to be carried out on the NSF funded Extreme Science and Engineering Discovery Environment (XSEDE) cloud computing resource Jetstream. Jetstream is a unique user-friendly cloud computing resource in that it provides the students with an environment with root privileges, making program installation less complicated. Jetstream also provides a Desktop simulator which allows students to transition between bash and R using the same exact directory paths as the instructor, reducing the confusion of students having unique file paths and providing a more streamlined approach to what for many students is their first coding class. Jetstream educational allocations can be applied for through XSEDE. The process of applying for a jetstream allocation for educational purposes is relatively straightforward and simply requires a course description, the number of students anticipated to participate, and a calculation of the computing resources required, and the instructors CV. Allocations are typically approved in two to three days and can be submitted ~2 weeks prior to the start of the course. Alternatively, these course materials can be used on students and instructors individual laptops. The file paths will need to be modified and we cannot guarantee that the installation of programs and R packages will proceed as they did on Jetstream. Thus, progam installation will need to be verified for each week by the instructor. 1.2 Assessment We will be using a transparent module-based form of assessment. For each section of material covered in the course, a module will be released which will consist of several quizzes with multiple-choice questions relevant to that modules material. These modules will reinforce your understanding of the material and you will be able to take each quiz multiple times to ensure accurate completion. There will be nine modules throughout the course. You will also have the option of submitting short recorded demonstrations that can be recorded through Zoom, for additional graded credit. There is no need to have sound or video showing during these recordings; further instructions will be provided on how to make and submit a recording. The option of submitting a final recorded project demonstration that will showcase multiple skills learned throughout the course will be discussed later in the course, for maximum graded credit. grade breakdown 1.3 Short recorded demonstrations The purpose of these demonstrations is to showcase the skills you have learned in the course. There will be a list of possible demonstrations for each module once the assessment for that module is released. These demonstrations will include no more than four related skills per recording. For example, in the bash/UNIX module you could record a demonstration of yourself changing directories, making a new directory, and generating a new file in that new directory, and editing that file. These recordings are expected to be no more than 5-10 minutes long (and many could be recorded in much less time). You are free to choose which module(s) you would like to prepare a demonstration for, with the only restriction being that your three recorded demonstrations must each come from a different module. 1.4 Final demonstration This demonstration will allow you to show that you have mastered the skills we have covered in class, but can also extend your abilities and complete somewhat novel analyses. These optional demonstrations focus on the final set of modules (#s 4-9) using skills in both bash and R, focusing on questions that we pose to you at the end of each module. For example, we may ask you to repeat an analysis that we did in class but with a different parameter, and then compare the difference between what we did in class and your results with the changed parameter. We will also require you to submit your code which can be saved as a simple text document (instructions on how to do this will be provided). For a sense of the types of questions that we will ask, please look at the end of each section of this website from a previous iteration of the course where we ended each week with Exercises/Practice Questions: https://baylab.github.io/MarineGenomics/ "],["week-1--welcome.html", "2 Week 1- Welcome! 2.1 Introduction to shell computing via the data carpentry tutorial 2.2 How to access the shell 2.3 Best practices for Jetstream 2.4 Week 1 Objectives 2.5 Navigating your file system 2.6 Shortcut: Tab Completion 2.7 Summary &amp; Key Points 2.8 Navigating Files and Directories 2.9 Moving around the file system 2.10 Navigating practice and finding hidden directories 2.11 Examining the contents of other directories 2.12 Full vs Relative Paths 2.13 Navigational shortcuts 2.14 Key Points 2.15 Creature of the Week!", " 2 Week 1- Welcome! Welcome to Marine Genomics Fall 2021 at Cal State LA! You will find the lecture for week one here 2.1 Introduction to shell computing via the data carpentry tutorial We will be following the data carpentry tutorial (Copyright 2016 @ Software Carpentry) Introduction to the command line for genomics. We have made some modifications to the data carpentry tutorial to fit our course. What is a shell and why should I care? A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface, or have extra capabilities in the command line version that are not available in the GUI. This is true, for example, of BLAST, which offers many advanced functions only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to do the same set of tasks with a large number of files. Learning the shell will allow you to automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), theyre likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that youve carried out, which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what youve done, so that others can check your work or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and cant realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell. In this lesson you will learn how to use the command line interface to move around in your file system. 2.2 How to access the shell For this course we will be using the shell in our Jetstream allocation through xsede. Jetstream is a cloud computing resource for which we have been allocated resources for the purposes of this course. Below is a guide for accessing and using jetstream. In jetstream we launch what they call an instance which is a small allocation that specifies how much memory you need and reflects how much computing you might do (well guide you through this). Youll find the jetstream login here Navigate to the login with xsede tab in the upper right This will redirect you to the xsede login page. Your organization should say xsede. Click continue The username for our course is margeno. The password will be given out in class. If you missed it please contact Serena Caplins (sacaplins@ucdavis.edu) or Maddie Armstrong (mlarmstrong@ucdavis.edu) Once youve logged in you should be redirected to jetstream. You now need to create your own projects folder. This is where you will carry out all of your analyses for the course. Everyone will get one project folder. So please only make one. If you make a mistake you can delete it and start again. Once you have a project folder we can create our first instance. Go to the New tab and select instance. Select the RStudio Desktop Shiny Server instance Select a small instance size if it isnt already selected (should be the default). It will take several minutes (5-10) to build our instance. Once it says active it will take a few more minutes to deploy. Once its ready to go you should see several actions on the right side of the screen including report, suspend, stop, etc. Select open web desktop. You should see something like the little desktop above. Its not pretty but this is where well be spending a lot of time. Select the black box on the bottom menu bar to access the command line. It will open a new window that has the $ prompt. 2.3 Best practices for Jetstream Once you launch your instance and its active its ready to use. When youre done coding its best to close your jetstream desktop and projects folder browser window. If you are going to step away from jetstream for a while (say over the weekend), its best to Shelve or Suspend your instance. Once youre ready to code again you just have to select Resume to start your instance again. This will take a few minutes. If in doubt of whether you should shelve/suspend or keep active, its our recommendation to shelve/suspend your instance. This frees up resources for other users and should reduce the likelihood of your active instance freezing up and being inaccessible. If you get an error while trying to log in or open the web shell or web desktop, follow these tips: log out of the jetstream home page and log back in relaunch your instance reboot your instance If those options fail then it is likely best to delete the instance and relaunch a new instance. This will result in you losing your data or work, but that should not be terribly detrimental to the course as everything resets with new data each week. 2.4 Week 1 Objectives Questions to Answer: How can I perform operations on files outside of my working directory? What are some navigational shortcuts I can use to make my work more efficient? Main Tasks: Use a single command to navigate multiple steps in your directory structure, including moving backwards (one level up). Perform operations on files in directories outside your working directory. Work with hidden directories and hidden files. Interconvert between absolute and relative paths. Employ navigational shortcuts to move around your file system. 2.5 Navigating your file system The part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called folders), which hold files or other directories. Several commands are frequently used to create, inspect, rename, and delete files and directories. $ The dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, either from these lessons or from other sources, do not type the prompt, only the commands that follow it. Lets find out where we are by running a command called pwd (which stands for print working directory). At any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in, unless we explicitly specify something else. Here, the computers response is /home/margeno, which is the top level directory within our cloud system: $ pwd /home/margeno Lets look at how our file system is organized. We can see what files and subdirectories are in this directory by running ls, which stands for listing: $ ls Desktop Documents Downloads Music Pictures Public Templates Videos ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. Well make a new subdirectory MarineGenomics where we will be creating new subdirectories, throughout this workshop. To make a new directory type the command mkdir followed by the name of the directory, in this case MarineGenomics. $ mkdir MarineGenomics Check that its there with ls $ ls Desktop Documents Downloads Music Pictures Public Templates Videos MarineGenomics The command to change locations in our file system is cd, followed by a directory name to change our working directory. cd stands for change directory. Lets say we want to navigate to the MarineGenomics directory we saw above. We can use the following command to get there: $ cd MarineGenomics $ pwd /home/margeno/MarineGenomics Use ls to see what is inside MarineGenomics $ ls It should be empty because we just created it and havent put anything in it yet. Lets download some data to work with. Well put it in our MarineGenomics directory. Lets first navigate back to our home directory. cd There are many ways to do transfer files and download data. We can use the command wget which needs a link to the file that we want to download. If theres a file saved on a website somewhere (anywhere on the internet) wget will download it for you. $ wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week1.tar.gz We now need to untar that file so we can access the contents tar -xzvf week1.tar.gz Now that we have something in our MarineGenomics directory we can use the ls command a bit more. We can make the ls output more comprehensible by using the flag -F, which tells ls to add a trailing / to the names of directories: $ ls -F week1/ Great, its there! Lets cd into the data directory and then use ls to see whats in that directory. $ cd week1 $ ls -F Week1/ Anything with a / after it is a directory. Things with a * after them are programs. If there are no decorations, its a file. ls has lots of other options. To find out what they are, we can type: $ man ls man (short for manual) displays detailed documentation (also referred as man page or man file) for bash commands. It is a powerful resource to explore bash commands, understand their usage and flags. Some manual files are very long. You can scroll through the file using your keyboards down arrow or use the Space key to go forward one page and the b key to go backwards one page. When you are done reading, hit q to quit. Use the -l option for the ls command to display more information for each item in the directory. What is one piece of additional information this long format gives you that you dont see with the bare ls command? $ ls -l No one can possibly learn all of these arguments, thats what the manual page is for. You can (and should) refer to the manual page or other help files as needed. Lets go into the Week1 directory and see what is in there. $ cd week1 $ ls -F SRR6805880_1.fastq SRR6805880_2.fastq This directory contains two files with .fastq extensions. FASTQ is a format for storing information about sequencing reads and their quality. We will be learning more about FASTQ files in a later lesson. 2.6 Shortcut: Tab Completion Typing out file or directory names can waste a lot of time and its easy to make typing mistakes. Instead we can use tab complete as a shortcut. When you start typing out the name of a directory or file, then hit the Tab key, the shell will try to fill in the rest of the directory or file name. Return to your home directory: $ cd then enter $ cd Mar&lt;tab&gt; The shell will fill in the rest of the directory name for MarineGenomics. Now change directories to Week1 in data in MarineGenomics $ cd MarineGenomics $ cd data $ cd Week1 Using tab complete can be very helpful. However, it will only autocomplete a file or directory name if youve typed enough characters to provide a unique identifier for the file or directory you are trying to access. For example, if we now try to list the files which names start with SR by using tab complete: $ ls SR&lt;tab. The shell auto-completes your command to SRR6805880_, because all file names in the directory begin with this prefix. When you hit Tab again, the shell will list the possible choices. $ ls SRR68&lt;tab&gt;&lt;tab&gt; SRR6805880_1.fastq SRR6805880_2.fastq Tab completion can also fill in the names of programs, which can be useful if you remember the beginning of a program name. $ pw&lt;tab&gt;&lt;tab&gt; pwck pwconv pwd pwdx pwunconv Displays the name of every program that starts with pw. 2.7 Summary &amp; Key Points We now know how to move around our file system using the command line. This gives us an advantage over interacting with the file system through a GUI as it allows us to work on a remote server, carry out the same set of operations on a large number of files quickly, and opens up many opportunities for using bioinformatic software that is only available in command line versions. In the next few episodes, well be expanding on these skills and seeing how using the command line shell enables us to make our workflow more efficient and reproducible. The shell gives you the ability to work more efficiently by using keyboard commands rather than a GUI. Useful commands for navigating your file system include: ls, pwd, and cd. Most commands take options (flags) which begin with a -. Tab completion can reduce errors from mistyping and make work more efficient in the shell. 2.8 Navigating Files and Directories This continues the shell module from Data Carpentrys introduction to the shell, which can be found here https://datacarpentry.org/shell-genomics/02-the-filesystem/index.html 2.9 Moving around the file system Weve learned how to use pwd to find our current location within our file system. Weve also learned how to use cd to change locations and ls to list the contents of a directory. Now were going to learn some additional commands for moving around within our file system. Use the commands weve learned so far to navigate to the MarineGenomics/data/Week1 directory, if youre not already there. $ cd $ cd MarineGenomics $ cd Week1 What if we want to move back up and out of this directory and to our top level directory? Can we type cd MarineGenomics? Try it and see what happens. $ cd MarineGenomics -bash: cd: MarineGenomics: No such file or directory Your computer looked for a directory or file called MarineGenomics within the directory you were already in. It didnt know you wanted to look at a directory level above the one you were located in. We have a special command to tell the computer to move us back or up one directory level. $ cd .. Now we can use pwd to make sure that we are in the directory we intended to navigate to, and ls to check that the contents of the directory are correct. $ pwd home/margeno/MarineGenomics $ ls data From this output, we can see that .. did indeed take us back one level in our file system. You can chain these together like so: $ ls ../../ prints the contents of /home. 2.10 Navigating practice and finding hidden directories Go back to your home directory. From there list the contents of the home directory. There are hidden directories in our home directory. Explore the options for ls to find out how to see hidden directories. List the contents of the directory and identify the name of the text file in that directory. Hint: hidden files and folders in Unix start with ., for example .my_hidden_directory Solution $ cd Lets look at some of the options for the ls function using the man command (note this will print out several lines of text) $ man ls The -a option is short for all and says that it causes ls to not ignore entries starting with . This is the option we want. $ ls -a . .ICEauthority .Rhistory .ansible .bash_logout .cache .dbus .fontconfig .local .r .ssh .wget-hsts Desktop Downloads Pictures Templates .. .Renviron .Xauthority .bash_history .bashrc .config .emacs.d .gnupg .profile .rstudio-desktop .vnc .xsession-errors Documents Music Public Videos Youll see there are many more files shown now that we can look at the hidden ones. In most commands the flags can be combined together in no particular order to obtain the desired results/output. $ ls -Fa $ ls -laF 2.11 Examining the contents of other directories By default, the ls commands lists the contents of the working directory (i.e. the directory you are in). You can always find the directory you are in using the pwd command. However, you can also give ls the names of other directories to view. Navigate to your home directory if you are not already there. $ cd Then enter the command: $ ls MarineGenomics data This will list the contents of the MarineGenomics directory without you needing to navigate there. The cd command works in a similar way. Try entering: $ cd $ cd MarineGenomics/week1 This will take you to the MarineGenomics directory without having to go through the intermediate directory. Navigating practice Navigate to your home directory. From there, list the contents of the Week1 directory. $ cd $ ls MarineGenomics/Week1 SRR6805880_1.fastq SRR6805880_2.fastq 2.12 Full vs Relative Paths The cd command takes an argument which is a directory name. Directories can be specified using either a relative path or a full absolute path. The directories on the computer are arranged into a hierarchy. The full path tells you where a directory is in that hierarchy. Navigate to the home directory, then enter the pwd command. $ cd $ pwd You should see: /home/margeno This is the full name of your home directory. This tells you that you are in a directory called margeno, which sits inside a directory called home which sits inside the very top directory in the hierarchy. The very top of the hierarchy is a directory called / which is usually referred to as the root directory. So, to summarize: margeno is a directory in home which is a directory in /. More on root and home in the next section. Now enter the following command: $ cd /home/margeno/MarineGenomics/Week1 This jumps several levels to the Week1 directory. Now go back to the home directory. $ cd You can also navigate to the Week1 directory using: $ cd MarineGenomics/Week1 These two commands have the same effect, they both take us to the Week1 directory. The first uses the absolute path, giving the full address from the home directory. The second uses a relative path, giving only the address from the working directory. A full path always starts with a /. A relative path does not. A relative path is like getting directions from someone on the street. They tell you to go right at the stop sign, and then turn left on Main Street. That works great if youre standing there together, but not so well if youre trying to tell someone how to get there from another country. A full path is like GPS coordinates. It tells you exactly where something is no matter where you are right now. You can usually use either a full path or a relative path depending on what is most convenient. If we are in the home directory, it is more convenient to enter the full path. If we are in the working directory, it is more convenient to enter the relative path since it involves less typing. Over time, it will become easier for you to keep a mental note of the structure of the directories that you are using and how to quickly navigate amongst them. 2.13 Navigational shortcuts The root directory is the highest level directory in your file system and contains files that are important for your computer to perform its daily work. While you will be using the root (/) at the beginning of your absolute paths, it is important that you avoid working with data in these higher-level directories, as your commands can permanently alter files that the operating system needs to function. In many cases, trying to run commands in root directories will require special permissions which will be discussed later, so its best to avoid them and work within your home directory. Dealing with the home directory is very common. The tilde character, ~, is a shortcut for your home directory. In our case, the root directory is two levels above our home directory, so cd or cd ~ will take you to /home/margeno and cd / will take you to /. Navigate to the MarineGenomics directory: $ cd $ cd MarineGenomics Then enter the command: $ ls ~ Desktop Documents Downloads Music Pictures Public Templates Videos MarineGenomics This prints the contents of your home directory, without you needing to type the full path. The commands cd, and cd ~ are very useful for quickly navigating back to your home directory. We will be using the ~ character in later lessons to specify our home directory. 2.14 Key Points The /, ~, and .. characters represent important navigational shortcuts. Hidden files and directories start with . and can be viewed using ls -a. Relative paths specify a location starting from the current location, while absolute paths specify a location from the root of the file system. 2.15 Creature of the Week! Spirula spirula! "],["week-2-working-with-files.html", "3 Week 2: Working With Files 3.1 Our data set: FASTQ files 3.2 Wildcards 3.3 Exercise 3.4 Using echo 3.5 Command History 3.6 Examining Files 3.7 Exercise 3.8 Exercise 3.9 Details on the FASTQ format 3.10 Creating, moving, copying, and removing 3.11 File Permissions 3.12 Removing 3.13 Exercise 3.14 Redirections 3.15 Searching files 3.16 Exercise 3.17 Redirecting output 3.18 Writing for loops 3.19 Using Basename in for loops 3.20 Exercise 3.21 Exercise 3.22 Writing Scripts and Working with Data 3.23 Writing files 3.24 Exercise 3.25 Writing scripts 3.26 Exercise 3.27 Making the script into a program 3.28 Creature of the Week!", " 3 Week 2: Working With Files Before we start, make sure to clone or update the github folder MarineGenomics in the user directory cd /home/margeno/ wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week2.tar.gz use tar to uncompress and unzip the file tar -xzvf week2.tar.gz Questions: How can I view and search file contents? How can I create, copy and delete files and directories? How can I control who has permission to modify a file? How can I repeat recently used commands? Objectives: View, search within, copy, move, and rename files. Create new directories. Use wildcards (*) to perform operations on multiple files. Make a file read only. Use the history command to view and repeat recently used commands. Keypoints: You can view file contents using less, cat, head or tail. The commands cp, mv, and mkdir are useful for manipulating existing files and creating new directories. You can view file permissions using ls -l and change permissions using chmod. The history command and the up arrow on your keyboard can be used to repeat recently used commands. 3.1 Our data set: FASTQ files Now that we know how to navigate around our directory structure, lets start working with our sequencing files. We did a sequencing experiment and have two results files, which are stored in our untrimmed_fastq directory. 3.2 Wildcards Navigate to your untrimmed_fastq directory: $ cd ~/MarineGenomics/Week2/untrimmed_fastq We are interested in looking at the FASTQ files in this directory. We can list all files with the .fastq extension using the command: $ ls *.fastq SRR097977.fastq SRR098026.fastq The * character is a special type of character called a wildcard, which can be used to represent any number of any type of character. Thus, *.fastq matches every file that ends with .fastq. This command: $ ls *977.fastq SRR097977.fastq lists only the file that ends with 977.fastq. This command: $ ls /usr/bin/*.sh /usr/bin/gettext.sh /usr/bin/rescan-scsi-bus.sh Lists every file in /usr/bin that ends in the characters .sh. Note that the output displays full paths to files, since each result starts with /. 3.3 Exercise Do each of the following tasks from your current directory using a single ls command for each: List all of the files in /usr/bin that start with the letter c. List all of the files in /usr/bin that contain the letter a. List all of the files in /usr/bin that end with the letter o. Bonus: List all of the files in /usr/bin that contain the letter a or the letter c. Hint: The bonus question requires a Unix wildcard that we havent talked about yet. Try searching the internet for information about Unix wildcards to find what you need to solve the bonus problem. Solution ls /usr/bin/c* ls /usr/bin/*a* ls /usr/bin/*o Bonus: ls /usr/bin/*[ac]* 3.4 Using echo echo is a built-in shell command that writes its arguments, like a line of text to standard output. The echo command can also be used with pattern matching characters, such as wildcard characters. Here we will use the echo command to see how the wildcard character is interpreted by the shell. $ echo *.fastq SRR097977.fastq SRR098026.fastq The * is expanded to include any file that ends with .fastq. We can see that the output of echo *.fastq is the same as that of ls *.fastq. What would the output look like if the wildcard could not be matched? Compare the outputs of echo *.missing and ls *.missing. Solution $ echo *.missing *.missing $ ls *.missing ls: cannot access &#39;*.missing&#39;: No such file or directory 3.5 Command History If you want to repeat a command that youve run recently, you can access previous commands using the up arrow on your keyboard to go back to the most recent command. Likewise, the down arrow takes you forward in the command history. A few more useful shortcuts: Ctrl+C will cancel the command you are writing, and give you a fresh prompt. Ctrl+R will do a reverse-search through your command history. This is very useful. Ctrl+L or the clear command will clear your screen. You can also review your recent commands with the history command, by entering: $ history to see a numbered list of recent commands. You can reuse one of these commands directly by referring to the number of that command. For example, if your history looked like this: 259 ls * 260 ls /usr/bin/*.sh 261 ls *R1*fastq then you could repeat command #260 by entering: $ !260 Type ! (exclamation point) and then the number of the command from your history. You will be glad you learned this when you need to re-run very complicated commands. For more information on advanced usage of history, read section 9.3 of Bash manual. You can also do this with Ctrl+R and a search for ls which may be more of less usefull depending on how often you use the command ls 3.6 Examining Files We now know how to switch directories, run programs, and look at the contents of directories, but how do we look at the contents of files? One way to examine a file is to print out all of the contents using the program cat. Enter the following command from within the untrimmed_fastq directory: $ cat SRR098026.fastq This will print out all of the contents of the SRR098026.fastq to the screen. 3.7 Exercise Print out the contents of the ~/MarineGenomics/week2/untrimmed_fastq/SRR097977.fastq file. What is the last line of the file? From your home directory, and without changing directories, use one short command to print the contents of all of the files in the ./MarineGenomics/Week2/untrimmed_fastq directory. Solution The last line of the file is C:CCC::CCCCCCCC&lt;8?6A:C28C&lt;608'&amp;&amp;&amp;,'$. cat ./data_week2/Week2/data_week2/untrimmed_fastq/* cat is a terrific program, but when the file is really big, it can be annoying to use. The program, less, is useful for this case. less opens the file as read only, and lets you navigate through it. The navigation commands are identical to the man program. Enter the following command: $ less SRR097977.fastq Some navigation commands in less: key action Space to go forward b to go backward g to go to the beginning G to go to the end q to quit less also gives you a way of searching through files. Use the / key to begin a search. Enter the word you would like to search for and press enter. The screen will jump to the next location where that word is found. Shortcut: If you hit / then enter, less will repeat the previous search. less searches from the current location and works its way forward. Scroll up a couple lines on your terminal to verify you are at the beginning of the file. Note, if you are at the end of the file and search for the sequence CAA, less will not find it. You either need to go to the beginning of the file (by typing g) and search again using / or you can use ? to search backwards in the same way you used / previously. For instance, lets search forward for the sequence TTTTT in our file. You can see that we go right to that sequence, what it looks like, and where it is in the file. If you continue to type / and hit return, you will move forward to the next instance of this sequence motif. If you instead type ? and hit return, you will search backwards and move up the file to previous examples of this motif. 3.8 Exercise What are the next three nucleotides (characters) after the first instance of the sequence quoted above? Solution CAC Remember, the man program actually uses less internally and therefore uses the same commands, so you can search documentation using / as well! Theres another way that we can look at files, and in this case, just look at part of them. This can be particularly useful if we just want to see the beginning or end of the file, or see how its formatted. The commands are head and tail and they let you look at the beginning and end of a file, respectively. $ head SRR098026.fastq @SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 NNNNNNNNNNNNNNNNCNNNNNNNNNNNNNNNNNN +SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 !!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!! @SRR098026.2 HWUSI-EAS1599_1:2:1:0:312 length=35 NNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN +SRR098026.2 HWUSI-EAS1599_1:2:1:0:312 length=35 !!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!! @SRR098026.3 HWUSI-EAS1599_1:2:1:0:570 length=35 NNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN $ tail SRR098026.fastq +SRR098026.247 HWUSI-EAS1599_1:2:1:2:1311 length=35 #!##!#################!!!!!!!###### @SRR098026.248 HWUSI-EAS1599_1:2:1:2:118 length=35 GNTGNGGTCATCATACGCGCCCNNNNNNNGGCATG +SRR098026.248 HWUSI-EAS1599_1:2:1:2:118 length=35 B!;?!A=5922:##########!!!!!!!###### @SRR098026.249 HWUSI-EAS1599_1:2:1:2:1057 length=35 CNCTNTATGCGTACGGCAGTGANNNNNNNGGAGAT +SRR098026.249 HWUSI-EAS1599_1:2:1:2:1057 length=35 A!@B!BBB@ABAB#########!!!!!!!###### The -n option to either of these commands can be used to print the first or last n lines of a file. $ head -n 1 SRR098026.fastq @SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 $ tail -n 1 SRR098026.fastq A!@B!BBB@ABAB#########!!!!!!!###### 3.9 Details on the FASTQ format Although it looks complicated (and it is), its easy to understand the fastq format with a little decoding. Some rules about the format include Line Description 1 Always begins with @ and then information about the read 2 The actual DNA sequence 3 Always begins with a + and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 We can view the first complete read in one of the files in our dataset by using head to look at the first four lines. $ head -n 4 SRR098026.fastq @SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 NNNNNNNNNNNNNNNNCNNNNNNNNNNNNNNNNNN +SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 !!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!! All but one of the nucleotides in this read are unknown (N). This is a pretty bad read! Line 4 shows the quality for each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g. 1 in 10) or, equivalently, the base call accuracy (e.g. 90%). To make it possible to line up each individual nucleotide with its quality score, the numerical score is converted into a code where each individual character represents the numerical quality score for an individual nucleotide. For example, in the line above, the quality score line is: !!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!! The # character and each of the ! characters represent the encoded quality for an individual nucleotide. The numerical value assigned to each of these characters depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses the standard Sanger quality PHRED score encoding, Illumina version 1.8 onwards. Here is a link showing what those different symbols mean for quality scores: https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm Each character is assigned a quality score between 0 and 42 as shown in the chart below. Quality encoding: !&quot;#$%&amp;&#39;()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJK | | | | | Quality score: 0........10........20........30........40.. Each quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%. These probability values are the results from the base calling algorithm and dependent on how much signal was captured for the base incorporation. Looking back at our read: @SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 NNNNNNNNNNNNNNNNCNNNNNNNNNNNNNNNNNN +SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35 !!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!! we can now see that the quality of each of the Ns is 0 and the quality of the only nucleotide call (C) is also very poor (# = a quality score of 2). This is indeed a very bad read. 3.10 Creating, moving, copying, and removing Now we can move around in the file structure, look at files, and search files. But what if we want to copy files or move them around or get rid of them? Most of the time, you can do these sorts of file manipulations without the command line, but there will be some cases (like when youre working with a remote computer like we are for this lesson) where it will be impossible. Youll also find that you may be working with hundreds of files and want to do similar manipulations to all of those files. In cases like this, its much faster to do these operations at the command line. 3.10.1 Copying Files When working with computational data, its important to keep a safe copy of that data that cant be accidentally overwritten or deleted. For this lesson, our raw data is our FASTQ files. We dont want to accidentally change the original files, so well make a copy of them and change the file permissions so that we can read from, but not write to, the files. First, lets make a copy of one of our FASTQ files using the cp command. Navigate to the ~/MarineGenomics/week2/untrimmed_fastq directory and enter: $ cp SRR098026.fastq SRR098026-copy.fastq $ ls -F SRR097977.fastq SRR098026-copy.fastq SRR098026.fastq We now have two copies of the SRR098026.fastq file, one of them named SRR098026-copy.fastq. Well move this file to a new directory called backup where well store our backup data files. 3.10.2 Creating Directories The mkdir command is used to make a directory. Enter mkdir followed by a space, then the directory name you want to create: $ mkdir backup 3.10.3 Moving / Renaming We can now move our backup file to this directory. We can move files around using the command mv: $ mv SRR098026-copy.fastq backup $ ls backup SRR098026-copy.fastq The mv command is also how you rename files. Lets rename this file to make it clear that this is a backup: $ cd backup $ mv SRR098026-copy.fastq SRR098026-backup.fastq $ ls SRR098026-backup.fastq 3.11 File Permissions Weve now made a backup copy of our file, but just because we have two copies, it doesnt make us safe. We can still accidentally delete or overwrite both copies. To make sure we cant accidentally mess up this backup file, were going to change the permissions on the file so that were only allowed to read (i.e. view) the file, not write to it (i.e. make new changes). View the current permissions on a file using the -l (long) flag for the ls command: $ ls -l -rw-rw-r-- 1 margeno margeno 43K Apr 6 12:15 SRR098026-backup.fastq The first part of the output for the -l flag gives you information about the files current permissions. There are ten slots in the permissions list. The first character in this list is related to file type, not permissions, so well ignore it for now. The next three characters relate to the permissions that the file owner has, the next three relate to the permissions for group members, and the final three characters specify what other users outside of your group can do with the file. Were going to concentrate on the three positions that deal with your permissions (as the file owner). Permissions breakdown Here the three positions that relate to the file owner are rw-. The r means that you have permission to read the file, the w indicates that you have permission to write to (i.e. make changes to) the file, and the third position is a -, indicating that you dont have permission to carry out the ability encoded by that space (this is the space where x or executable ability is stored, well talk more about this in a later lesson). Our goal for now is to change permissions on this file so that you no longer have w or write permissions. We can do this using the chmod (change mode) command and subtracting (-) the write permission -w. $ chmod -w SRR098026-backup.fastq $ ls -l -r--r--r-- 1 dcuser dcuser 43332 Nov 15 23:02 SRR098026-backup.fastq Chmod can also change the permission to only the user (u), group (g), and/or other (o). Lets add reading (r), writing (x), and execute (x) permissions to user group. $ chmod u=rwx SRR098026-backup.fastq $ ls -l 3.12 Removing To prove to ourselves that you no longer have the ability to modify this file, try deleting it with the rm command: $ rm SRR098026-backup.fastq Youll be asked if you want to override your file permissions: rm: remove write-protected regular file SRR098026-backup.fastq? You should enter n for no. If you enter n (for no), the file will not be deleted. If you enter y, you will delete the file. This gives us an extra measure of security, as there is one more step between us and deleting our data files. Important: The rm command permanently removes the file. Be careful with this command. It doesnt just nicely put the files in the Trash. Theyre really gone. By default, rm will not delete directories. You can tell rm to delete a directory using the -r (recursive) option. Lets delete the backup directory we just made. Enter the following command: $ cd .. $ rm -r backup This will delete not only the directory, but all files within the directory. If you have write-protected files in the directory, you will be asked whether you want to override your permission settings. 3.13 Exercise Starting in the ~/MarineGenomics/Week2/untrimmed_fastq/ directory, do the following: 1. Make sure that you have deleted your backup directory and all files it contains. 2. Create a backup of each of your FASTQ files using cp. (Note: Youll need to do this individually for each of the two FASTQ files. We havent learned yet how to do this with a wildcard.) 3. Use a wildcard to move all of your backup files to a new backup directory. 4. Change the permissions on all of your backup files to be write-protected. Solution rm -r backup cp SRR098026.fastq SRR098026-backup.fastq and cp SRR097977.fastq SRR097977-backup.fastq mkdir backup and mv *-backup.fastq backup chmod -w backup/*-backup.fastq Its always a good idea to check your work with ls -l backup. You should see something like: -r--r--r-- 1 dcuser dcuser 47552 Nov 15 23:06 SRR097977-backup.fastq -r--r--r-- 1 dcuser dcuser 43332 Nov 15 23:06 SRR098026-backup.fastq 3.14 Redirections Questions: How can I search within files? How can I combine existing commands to do new things? Objectives: Employ the grep command to search for information within files. Print the results of a command to a file. Construct command pipelines with two or more stages. Use for loops to run the same command for several input files. Keypoints: grep is a powerful search tool with many options for customization. &gt;, &gt;&gt;, and | are different ways of redirecting output. command &gt; file redirects a commands output to a file. command &gt;&gt; file redirects a commands output to a file without overwriting the existing contents of the file. command_1 | command_2 redirects the output of the first command as input to the second command. for loops are used for iteration. basename gets rid of repetitive parts of names. 3.15 Searching files We discussed in a previous episode how to search within a file using less. We can also search within files without even opening them, using grep. grep is a command-line utility for searching plain-text files for lines matching a specific set of characters (sometimes called a string) or a particular pattern (which can be specified using something called regular expressions). Were not going to work with regular expressions in this lesson, and are instead going to specify the strings we are searching for. Lets give it a try! Nucleotide abbreviations The four nucleotides that appear in DNA are abbreviated A, C, T and G. Unknown nucleotides are represented with the letter N. An N appearing in a sequencing file represents a position where the sequencing machine was not able to confidently determine the nucleotide in that position. You can think of an N as being aNy nucleotide at that position in the DNA sequence. Well search for strings inside of our fastq files. Lets first make sure we are in the correct directory: $ cd ~/MarineGenomics/Week2/untrimmed_fastq Suppose we want to see how many reads in our file have really bad segments containing 10 consecutive unknown nucleotides (Ns). Determining quality In this lesson, were going to be manually searching for strings of Ns within our sequence results to illustrate some principles of file searching. It can be really useful to do this type of searching to get a feel for the quality of your sequencing results, however, in your research you will most likely use a bioinformatics tool that has a built-in program for filtering out low-quality reads. Youll learn how to use one such tool in a later lesson. Lets search for the string NNNNNNNNNN in the SRR098026 file: $ grep NNNNNNNNNN SRR098026.fastq This command returns a lot of output to the terminal. Every single line in the SRR098026 file that contains at least 10 consecutive Ns is printed to the terminal, regardless of how long or short the file is. We may be interested not only in the actual sequence which contains this string, but in the name (or identifier) of that sequence. We discussed in a previous lesson that the identifier line immediately precedes the nucleotide sequence for each read in a FASTQ file. We may also want to inspect the quality scores associated with each of these reads. To get all of this information, we will return the line immediately before each match and the two lines immediately after each match. We can use the -B argument for grep to return a specific number of lines before each match. The -A argument returns a specific number of lines after each matching line. Here we want the line before and the two lines after each matching line, so we add -B1 -A2 to our grep command: $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq One of the sets of lines returned by this command is: @SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35 CNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN +SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 3.16 Exercise Search for the sequence GNATNACCACTTCC in the SRR098026.fastq file. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match. Search for the sequence AAGTT in both FASTQ files. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match. Solution grep -B1 GNATNACCACTTCC SRR098026.fastq @SRR098026.245 HWUSI-EAS1599_1:2:1:2:801 length=35 GNATNACCACTTCCAGTGCTGANNNNNNNGGGATG grep -B1 AAGTT *.fastq SRR097977.fastq-@SRR097977.11 209DTAAXX_Lenski2_1_7:8:3:247:351 length=36 SRR097977.fastq:GATTGCTTTAATGAAAAAGTCATATAAGTTGCCATG -- SRR097977.fastq-@SRR097977.67 209DTAAXX_Lenski2_1_7:8:3:544:566 length=36 SRR097977.fastq:TTGTCCACGCTTTTCTATGTAAAGTTTATTTGCTTT -- SRR097977.fastq-@SRR097977.68 209DTAAXX_Lenski2_1_7:8:3:724:110 length=36 SRR097977.fastq:TGAAGCCTGCTTTTTTATACTAAGTTTGCATTATAA -- SRR097977.fastq-@SRR097977.80 209DTAAXX_Lenski2_1_7:8:3:258:281 length=36 SRR097977.fastq:GTGGCGCTGCTGCATAAGTTGGGTTATCAGGTCGTT -- SRR097977.fastq-@SRR097977.92 209DTAAXX_Lenski2_1_7:8:3:353:318 length=36 SRR097977.fastq:GGCAAAATGGTCCTCCAGCCAGGCCAGAAGCAAGTT -- SRR097977.fastq-@SRR097977.139 209DTAAXX_Lenski2_1_7:8:3:703:655 length=36 SRR097977.fastq:TTTATTTGTAAAGTTTTGTTGAAATAAGGGTTGTAA -- SRR097977.fastq-@SRR097977.238 209DTAAXX_Lenski2_1_7:8:3:592:919 length=36 SRR097977.fastq:TTCTTACCATCCTGAAGTTTTTTCATCTTCCCTGAT -- SRR098026.fastq-@SRR098026.158 HWUSI-EAS1599_1:2:1:1:1505 length=35 SRR098026.fastq:GNNNNNNNNCAAAGTTGATCNNNNNNNNNTGTGCG 3.17 Redirecting output grep allowed us to identify sequences in our FASTQ files that match a particular pattern. All of these sequences were printed to our terminal screen, but in order to work with these sequences and perform other operations on them, we will need to capture that output in some way. We can do this with something called redirection. The idea is that we are taking what would ordinarily be printed to the terminal screen and redirecting it to another location. In our case, we want to print this information to a file so that we can look at it later and use other commands to analyze this data. The command for redirecting output to a file is &gt;. Lets try out this command and copy all the records (including all four lines of each record) in our FASTQ files that contain NNNNNNNNNN to another file called bad_reads.txt. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq &gt; bad_reads.txt File extensions You might be confused about why were naming our output file with a .txt extension. After all, it will be holding FASTQ formatted data that were extracting from our FASTQ files. Wont it also be a FASTQ file? The answer is, yes - it will be a FASTQ file and it would make sense to name it with a .fastq extension. However, using a .fastq extension will lead us to problems when we move to using wildcards later in this episode. Well point out where this becomes important. For now, its good that youre thinking about file extensions! The prompt should sit there a little bit, and then it should look like nothing happened. But type ls. You should see a new file called bad_reads.txt. We can check the number of lines in our new file using a command called wc. wc stands for word count. This command counts the number of words, lines, and characters in a file. The FASTQ file may change over time, so given the potential for updates, make sure your file matches your instructors output. As of Sept. 2020, wc gives the following output: $ wc bad_reads.txt 537 1073 23217 bad_reads.txt This will tell us the number of lines, words and characters in the file. If we want only the number of lines, we can use the -l flag for lines. $ wc -l bad_reads.txt 537 bad_reads.txt 3.17.1 Exercise How many sequences are there in SRR098026.fastq? Remember that every sequence is formed by four lines. Solution $ wc -l SRR098026.fastq 996 Now you can divide this number by four to get the number of sequences in your fastq file We might want to search multiple FASTQ files for sequences that match our search pattern. However, we need to be careful, because each time we use the &gt; command to redirect output to a file, the new output will replace the output that was already present in the file. This is called overwriting and, just like you dont want to overwrite your video recording of your kids first birthday party, you also want to avoid overwriting your data files. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq &gt; bad_reads.txt $ wc -l bad_reads.txt 537 bad_reads.txt $ grep -B1 -A2 NNNNNNNNNN SRR097977.fastq &gt; bad_reads.txt $ wc -l bad_reads.txt 0 bad_reads.txt Here, the output of our second call to wc shows that we no longer have any lines in our bad_reads.txt file. This is because the second file we searched (SRR097977.fastq) does not contain any lines that match our search sequence. So our file was overwritten and is now empty. We can avoid overwriting our files by using the command &gt;&gt;. &gt;&gt; is known as the append redirect and will append new output to the end of a file, rather than overwriting it. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq &gt; bad_reads.txt $ wc -l bad_reads.txt 537 bad_reads.txt $ grep -B1 -A2 NNNNNNNNNN SRR097977.fastq &gt;&gt; bad_reads.txt $ wc -l bad_reads.txt 537 bad_reads.txt The output of our second call to wc shows that we have not overwritten our original data. We can also do this with a single line of code by using a wildcard: $ grep -B1 -A2 NNNNNNNNNN *.fastq &gt; bad_reads.txt $ wc -l bad_reads.txt 537 bad_reads.txt File extensions - part 2 This is where we would have trouble if we were naming our output file with a .fastq extension. If we already had a file called bad_reads.fastq (from our previous grep practice) and then ran the command above using a .fastq extension instead of a .txt extension, grep would give us a warning. grep -B1 -A2 NNNNNNNNNN *.fastq &gt; bad_reads.fastq grep: input file bad_reads.fastq is also the output grep is letting you know that the output file bad_reads.fastq is also included in your grep call because it matches the *.fastq pattern. Be careful with this as it can lead to some unintended results. Since we might have multiple different criteria we want to search for, creating a new output file each time has the potential to clutter up our workspace. We also thus far havent been interested in the actual contents of those files, only in the number of reads that weve found. We created the files to store the reads and then counted the lines in the file to see how many reads matched our criteria. Theres a way to do this, however, that doesnt require us to create these intermediate files - the pipe command (|). What | does is take the output that is scrolling by on the terminal and uses that output as input to another command. When our output was scrolling by, we might have wished we could slow it down and look at it, like we can with less. Well it turns out that we can! We can redirect our output from our grep call through the less command. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | less We can now see the output from our grep call within the less interface. We can use the up and down arrows to scroll through the output and use q to exit less. If we dont want to create a file before counting lines of output from our grep search, we could directly pipe the output of the grep search to the command wc -l. This can be helpful for investigating your output if you are not sure you would like to save it to a file. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | wc -l Because we asked grep for all four lines of each FASTQ record, we need to divide the output by four to get the number of sequences that match our search pattern. Since 802 / 4 = 200.5 and we are expecting an integer number of records, there is something added or missing in bad_reads.txt. If we explore bad_reads.txt using less, we might be able to notice what is causing the uneven number of lines. Luckily, this issue happens by the end of the file so we can also spot it with tail. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq &gt; bad_reads.txt $ tail bad_reads.txt @SRR098026.133 HWUSI-EAS1599_1:2:1:0:1978 length=35 ANNNNNNNNNTTCAGCGACTNNNNNNNNNNGTNGN +SRR098026.133 HWUSI-EAS1599_1:2:1:0:1978 length=35 #!!!!!!!!!##########!!!!!!!!!!##!#! -- -- @SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35 CNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN +SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! The fifth and six lines in the output display  which is the default action for grep to separate groups of lines matching the pattern, and indicate groups of lines which did not match the pattern so are not displayed. To fix this issue, we can redirect the output of grep to a second instance of grep as follows. $ grep -B1 -A2 NNNNNNNNNN SRR098026.fastq | grep -v &#39;^--&#39; &gt; bad_reads.fastq tail bad_reads.fastq +SRR098026.132 HWUSI-EAS1599_1:2:1:0:320 length=35 #!!!!!!!!!##########!!!!!!!!!!##!#! @SRR098026.133 HWUSI-EAS1599_1:2:1:0:1978 length=35 ANNNNNNNNNTTCAGCGACTNNNNNNNNNNGTNGN +SRR098026.133 HWUSI-EAS1599_1:2:1:0:1978 length=35 #!!!!!!!!!##########!!!!!!!!!!##!#! @SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35 CNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN +SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! The -v option in the second grep search stands for --invert-match meaning grep will now only display the lines which do not match the searched pattern, in this case '^--'. The caret (^) is an anchoring character matching the beginning of the line, and the pattern has to be enclose by single quotes so grep does not interpret the pattern as an extended option (starting with ). Custom grep control Use man grep to read more about other options to customize the output of grep including extended options, anchoring characters, and much more. Redirecting output is often not intuitive, and can take some time to get used to. Once youre comfortable with redirection, however, youll be able to combine any number of commands to do all sorts of exciting things with your data! None of the command line programs weve been learning do anything all that impressive on their own, but when you start chaining them together, you can do some really powerful things very efficiently. 3.18 Writing for loops Loops are key to productivity improvements through automation as they allow us to execute commands repeatedly. Similar to wildcards and tab completion, using loops also reduces the amount of typing (and typing mistakes). Loops are helpful when performing operations on groups of sequencing files, such as unzipping or trimming multiple files. We will use loops for these purposes in subsequent analyses, but will cover the basics of them for now. When the shell sees the keyword for, it knows to repeat a command (or group of commands) once for each item in a list. Each time the loop runs (called an iteration), an item in the list is assigned in sequence to the variable, and the commands inside the loop are executed, before moving on to the next item in the list. Inside the loop, we call for the variables value by putting $ in front of it. The $ tells the shell interpreter to treat the variable as a variable name and substitute its value in its place, rather than treat it as text or an external command. In shell programming, this is usually called expanding the variable. Sometimes, we want to expand a variable without any whitespace to its right. Suppose we have a variable named foo that contains the text abc, and would like to expand foo to create the text abcEFG. $ foo=abc $ echo foo is $foo foo is abc $ echo foo is $fooEFG # doesn&#39;t work foo is The interpreter is trying to expand a variable named fooEFG, which (probably) doesnt exist. We can avoid this problem by enclosing the variable name in braces ({ and }, sometimes called squiggle braces). bash treats the # character as a comment character. Any text on a line after a # is ignored by bash when evaluating the text as code. $ foo=abc $ echo foo is $foo foo is abc $ echo foo is ${foo}EFG # now it works! foo is abcEFG Lets write a for loop to show us the first two lines of the fastq files we downloaded earlier. You will notice the shell prompt changes from $ to &gt; and back again as we were typing in our loop. The second prompt, &gt;, is different to remind us that we havent finished typing a complete command yet. A semicolon, ;, can be used to separate two commands written on a single line. $ cd ~/MarineGenomics/week2/untrimmed_fastq $ for filename in *.fastq &gt; do &gt; head -n 2 ${filename} &gt; done The for loop begins with the formula for &lt;variable&gt; in &lt;group to iterate over&gt;. In this case, the word filename is designated as the variable to be used over each iteration. In our case SRR097977.fastq and SRR098026.fastq will be substituted for filename because they fit the pattern of ending with .fastq in the directory weve specified. The next line of the for loop is do. The next line is the code that we want to execute. We are telling the loop to print the first two lines of each variable we iterate over. Finally, the word done ends the loop. After executing the loop, you should see the first two lines of both fastq files printed to the terminal. Lets create a loop that will save this information to a file. $ for filename in *.fastq &gt; do &gt; head -n 2 ${filename} &gt;&gt; seq_info.txt &gt; done When writing a loop, you will not be able to return to previous lines once you have pressed Enter. Remember that we can cancel the current command using Ctrl+C If you notice a mistake that is going to prevent your loop for executing correctly. Note that we are using &gt;&gt; to append the text to our seq_info.txt file. If we used &gt;, the seq_info.txt file would be rewritten every time the loop iterates, so it would only have text from the last variable used. Instead, &gt;&gt; adds to the end of the file. 3.19 Using Basename in for loops Basename is a function in UNIX that is helpful for removing a uniform part of a name from a list of files. In this case, we will use basename to remove the .fastq extension from the files that weve been working with. $ basename SRR097977.fastq .fastq We see that this returns just the SRR accession, and no longer has the .fastq file extension on it. SRR097977 If we try the same thing but use .fasta as the file extension instead, nothing happens. This is because basename only works when it exactly matches a string in the file. $ basename SRR097977.fastq .fasta SRR097977.fastq Basename is really powerful when used in a for loop. It allows to access just the file prefix, which you can use to name things. Lets try this. Inside our for loop, we create a new name variable. We call the basename function inside the parenthesis, then give our variable name from the for loop, in this case ${filename}, and finally state that .fastq should be removed from the file name. Its important to note that were not changing the actual files, were creating a new variable called name. The line &gt; echo $name will print to the terminal the variable name each time the for loop runs. Because we are iterating over two files, we expect to see two lines of output. $ for filename in *.fastq &gt; do &gt; name=$(basename ${filename} .fastq) &gt; echo ${name} &gt; done Exercise 3.20 Exercise Print the file prefix of all of the .txt files in our current directory. Solution $ for filename in *.txt &gt; do &gt; name=$(basename ${filename} .txt) &gt; echo ${name} &gt; done One way this is really useful is to move files. Lets rename all of our .txt files using mv so that they have the years on them, which will document when we created them. $ for filename in *.txt &gt; do &gt; name=$(basename ${filename} .txt) &gt; mv ${filename} ${name}_2021.txt &gt; done Exercise 3.21 Exercise Remove _2021 from all of the .txt files. Solution $ for filename in *_2021.txt &gt; do &gt; name=$(basename ${filename} _2021.txt) &gt; mv ${filename} ${name}.txt &gt; done 3.22 Writing Scripts and Working with Data Questions: How can we automate a commonly used set of commands? Objectives: Use the nano text editor to modify text files. Write a basic shell script. Use the bash command to execute a shell script. Use chmod to make a script an executable program. Keypoints: Scripts are a collection of commands executed together. Transferring information to and from virtual and local computers. 3.23 Writing files Weve been able to do a lot of work with files that already exist, but what if we want to write our own files? Were not going to type in a FASTA file, but well see as we go through other tutorials, there are a lot of reasons well want to write a file, or edit an existing file. To add text to files, were going to use a text editor called Nano. Were going to create a file to take notes about what weve been doing with the data files in ~/MarineGenomics/week2/untrimmed_fastq. This is good practice when working in bioinformatics. We can create a file called README.txt that describes the data files in the directory or documents how the files in that directory were generated. As the name suggests, its a file that we or others should read to understand the information in that directory. Lets change our working directory to ~/MarineGenomics/week2/untrimmed_fastq using cd, then run nano to create a file called README.txt: $ cd ~/MarineGenomics/week2/untrimmed_fastq $ nano README.txt You should see something like this: nano201711.png The text at the bottom of the screen shows the keyboard shortcuts for performing various tasks in nano. We will talk more about how to interpret this information soon. Which Editor? Which Editor? When we say, nano is a text editor, we really do mean text: it can only work with plain character data, not tables, images, or any other human-friendly media. We use it in examples because it is one of the least complex text editors. However, because of this trait, it may not be powerful enough or flexible enough for the work you need to do after this workshop. On Unix systems (such as Linux and Mac OS X), many programmers use Emacs or Vim (both of which require more time to learn), or a graphical editor such as Gedit. On Windows, you may wish to use Notepad++. Windows also has a built-in editor called notepad that can be run from the command line in the same way as nano for the purposes of this lesson. No matter what editor you use, you will need to know where it searches for and saves files. If you start it from the shell, it will (probably) use your current working directory as its default location. If you use your computers start menu, it may want to save files in your desktop or documents directory instead. You can change this by navigating to another directory the first time you Save As Lets type in a few lines of text. Describe what the files in this directory are or what youve been doing with them. Once were happy with our text, we can press Ctrl-O (press the Ctrl or Control key and, while holding it down, press the O key) to write our data to disk. Youll be asked what file we want to save this to: press Return to accept the suggested default of README.txt. Once our file is saved, we can use Ctrl-X to quit the editor and return to the shell. Control, Ctrl, or ^ Key The Control key is also called the Ctrl key. There are various ways in which using the Control key may be described. For example, you may see an instruction to press the Ctrl key and, while holding it down, press the X key, described as any of: Control-X Control+X Ctrl-X Ctrl+X ^X C-x In nano, along the bottom of the screen youll see ^G Get Help ^O WriteOut. This means that you can use Ctrl-G to get help and Ctrl-O to save your file. Now youve written a file. You can take a look at it with less or cat, or open it up again and edit it with nano. Exercise 3.24 Exercise Open README.txt and add the date to the top of the file and save the file. Solution Use nano README.txt to open the file. Add todays date and then use Ctrl-X followed by y and Enter to save. 3.25 Writing scripts A really powerful thing about the command line is that you can write scripts. Scripts let you save commands to run them and also lets you put multiple commands together. Though writing scripts may require an additional time investment initially, this can save you time as you run them repeatedly. Scripts can also address the challenge of reproducibility: if you need to repeat an analysis, you retain a record of your command history within the script. One thing we will commonly want to do with sequencing results is pull out bad reads and write them to a file to see if we can figure out whats going on with them. Were going to look for reads with long sequences of Ns like we did before, but now were going to write a script, so we can run it each time we get new sequences, rather than type the code in by hand each time. Were going to create a new file to put this command in. Well call it bad-reads-script.sh. The sh isnt required, but using that extension tells us that its a shell script. $ nano bad-reads-script.sh Bad reads have a lot of Ns, so were going to look for NNNNNNNNNN with grep. We want the whole FASTQ record, so were also going to get the one line above the sequence and the two lines below. We also want to look in all the files that end with .fastq, so were going to use the * wildcard. grep -B1 -A2 -h NNNNNNNNNN *.fastq | grep -v &#39;^--&#39; &gt; scripted_bad_reads.txt Custom grep control We introduced the -v option in the previous episode, now we are using -h to Suppress the prefixing of file names on output according to the documentation shown by man grep. Type your grep command into the file and save it as before. Be careful that you did not add the $ at the beginning of the line. Now comes the neat part. We can run this script. Type: $ bash bad-reads-script.sh It will look like nothing happened, but now if you look at scripted_bad_reads.txt, you can see that there are now reads in the file. 3.26 Exercise We want the script to tell us when its done. 1. Open bad-reads-script.sh and add the line echo \"Script finished!\" after the grep command and save the file. 2. Run the updated script. Solution $ bash bad-reads-script.sh Script finished! 3.27 Making the script into a program We had to type bash because we needed to tell the computer what program to use to run this script. Instead, we can turn this script into its own program. We need to tell it that its a program by making it executable. We can do this by changing the file permissions. We talked about permissions in an earlier episode. First, lets look at the current permissions. $ ls -l bad-reads-script.sh -rw-rw-r-- 1 margeno margeno 75 Apr 8 12:46 bad-reads-script.sh We see that it says -rw-r--r--. This shows that the file can be read by any user and written to by the file owner (you). We want to change these permissions so that the file can be executed as a program using ./ . We use the command chmod like we did earlier when we removed write permissions. Here we are adding (+) executable permissions (+x). $ chmod +x bad-reads-script.sh Now lets look at the permissions again. $ ls -l bad-reads-script.sh -rwxrwxr-x 1 margeno margeno 75 Apr 8 12:48 bad-reads-script.sh Now we see that it says -rwxr-xr-x. The xs that are there now tell us we can run it as a program. So, lets try it! Well need to put ./ at the beginning so the computer knows to look here in this directory for the program. $ ./bad-reads-script.sh The script should run the same way as before, but now weve created our very own computer program! 3.28 Creature of the Week! The Frilled shark! "],["week3-awk-and-how-to-download-files.html", "4 Week3: Awk and how to download files 4.1 AWK 4.2 Moving and Downloading Data 4.3 Getting data from the cloud 4.4 Downloading files from Github 4.5 Final bash wrap-up", " 4 Week3: Awk and how to download files For this week, well continue to use the data that you downloaded last week. If you need to download it again please use the wget link below to download the tar file. cd /home/margeno/ wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week2.tar.gz use tar to uncompress and unzip the file tar -xzvf week2.tar.gz This will create a week2 directory in your MarineGenomics directory (it will also create the MarineGenomics directory if you dont already have one). 4.1 AWK Awk is a fast and versatile pattern matching programming language. Awk can do the same tasks that sed, grep, cat, and wc; and then it can do a lot more https://www.gnu.org/software/gawk/manual/gawk.html. This program deserves a full class to go into details, so instead we just have this section to make you aware that the program exists. Lets see how awk can behave like wc. $ cd /home/margeno/MarineGenomics/week2/ $ ls TableS2_QTL_Bay_2017.txt sra_metadata untrimmed_fastq This table is from the Bay et al. 2017 publication ~/MarineGenomics/week2/TableS2_QTL_Bay_2017.txt and we will use it as our example file for this section. We can look inside the file by using cat or awk $ awk &#39;{print $0}&#39; TableS2_QTL_Bay_2017.txt The instructions are enclosed in single quotes This command has the same output of cat: it prints each line from the example file TableS2_QTL_Bay_2017.txt The structure of the instruction is the following: - curly braces surround the set of instructions - print is the instruction that sends its arguments to the terminal - $0 is a variable, it means the content of the current line As you can see, the file contains a table. Trait n LOD Chr Position (cM) Nearest SNP mate choice 200 4.5 14 22.43 chrXIV:1713227 mate choice 200 4.61 21 8 chrXXI:9373717 discriminant function 200 4.83 12 17 chrXII:7504339 discriminant function 200 4.23 14 8.1 chrXIV:4632223 PC2 200 4.04 4 30.76 chrIV:11367975 PC2 200 6.67 7 47 chrVII:26448674 centroid size 200 6.97 9 47.8 chrIX:19745222 x2* 200 3.93 7 60 chrUn:29400087 y2* 200 9.99 4 32 chrIV:11367975 x3 200 4.45 1 32.3 chrI:15145305 x4 200 5.13 16 30.9 chrXVI:12111717 x5* 200 4.54 15 6 chrXV:505537 y5 200 4.21 4 24.9 chrIV:15721538 x6 200 3.96 16 29.5 chrXVI:13588796 y6* 200 4.14 9 30.2 chrIX:18942598 y15* 200 5.3 2 27 chrII:19324477 x16 200 5.49 7 60 chrUn:29400087 x17 200 4.92 1 32.8 chrI:14261764 Table S2. Significant QTL loci for mate choice and morphology Now lets use awk to count the lines of a file, similarly to what wc -l would do. As you probably remember, -l is an option that asks for the number of lines only. However, wc counts the number of newlines in the file, if the last line does not contain a carriage return (i.e. there is no emptyline at the end of the file), the result is going be the actual number of lines minus one. $ wc -l TableS2_QTL_Bay_2017.txt 19 TableS2_QTL_Bay_2017.txt A workaround is to use awk. Awk is command line program that takes as input a set of instructions and one or more files. The instructions are executed on each line of the input file(s). $ awk &#39;{print NR;}&#39; TableS2_QTL_Bay_2017.txt | tail -1 Awk can also search within a file like grep can. Lets see if there are any significant QTL loci in the chromosome chrXIV $ awk &#39;/chrXIV/&#39; TableS2_QTL_Bay_2017.txt This chromosome had two significant QTL Loci for mate choice and morphology. When to use awk? for search and replacement of large files (its fast!) when manipulating multiple large files 4.2 Moving and Downloading Data Below well show you some commands to download data onto your instance, or to move data between your computer and the cloud. 4.3 Getting data from the cloud There are two programs that will download data from a remote server to your local (or remote) machine: wget and curl. They were designed to do slightly different tasks by default, so youll need to give the programs somewhat different options to get the same behaviour, but they are mostly interchangeable. wget is short for world wide web get, and its basic function is to download web pages or data at a web address. cURL is a pun, it is supposed to be read as see URL, so its basic function is to display webpages or data at a web address. Which one you need to use mostly depends on your operating system, as most computers will only have one or the other installed by default. Today we will use wget to download some data from Ensembl. Exercise Before we can start our download, we need to know whether were using curl or wget. To see which program you have, type: $ which curl $ which wget which is a BASH program that looks through everything you have installed, and tells you what folder it is installed to. If it cant find the program you asked for, it returns nothing, i.e. gives you no results. On Mac OSX, youll likely get the following output: $ which wget $ /usr/bin/wget Once you know whether you have curl or wget, use one of the following commands to download the file: $ cd $ wget ftp://ftp.ensemblgenomes.org/pub/release-37/bacteria/species_EnsemblBacteria.txt Lets see if the file from ensembl downloaded ls species_EnsemblBacteria.txt it did! 4.4 Downloading files from Github Github is a useful place to store data files and scripts and it is widely used by researchers in many different fields, including genomics. There are a few useful tricks to understanding how to best transfer files from github to your own terminal. There are two main ways to transfer files from github: + Use git clone to download an entire repository (Directory) + use wget to download a single file If youre interested in getting a repository and all of its contents you can use git clone. This can be useful if youre interested in using data files and the scripts that come with them. First navigate to your home directory with cd (leaving it blank takes you to your home automatically). And then use git clone to download the repository reallycoolrepo cd git clone https://github.com/SerenaCaplins/reallycoolrepo.git This should make a new directory called reallycoolrepo. Lets ls in this directory to see whats in it. ls reallycoolrepo/ files forloop.sh MarineGenomics.txt README.md We have three files and one directory here. You can view the MarineGenomics.txt file with cat cat reallycoolrepo/MarineGenomics.txt __ __ _ _____ _ | \\/ | (_) / ____| (_) | \\ / | __ _ _ __ _ _ __ ___ | | __ ___ _ __ ___ _ __ ___ _ ___ ___ | |\\/| | / _` | | &#39;__| | | | &#39;_ \\ / _ \\ | | |_ | / _ \\ | &#39;_ \\ / _ \\ | &#39;_ ` _ \\ | | / __| / __| | | | | | (_| | | | | | | | | | | __/ | |__| | | __/ | | | | | (_) | | | | | | | | | | (__ \\__ \\ |_| |_| \\__,_| |_| |_| |_| |_| \\___| \\_____| \\___| |_| |_| \\___/ |_| |_| |_| |_| \\___| |___/ Pretty cool huh? Using git clone to get an entire repository can be useful, but often were just interested in getting a single file. Weve already learned how to get files using wget, but this isnt as straightforward on git hub. To illustrate when it doesnt work lets navigate to a repository where theres a file that were interested in: copy and paste this link into your browser: https://github.com/BayLab/MarineGenomicsData This is the repository where we have been storing all of the data for the class. We typically download a single tar file each week instead of cloning the whole repository all at once (this allows us to make changes to each week without having to download the whole repo every week, which would also override your files). Say you wanted to get the week10.tar.gz file If you click on the file you can copy the file path from your browser. A few ways to do this but perhaps easiest is to click the file and copy the path that shows up in your browser. Week3_gitpath.jpg https://github.com/BayLab/MarineGenomicsData/blob/main/week10.tar.gz Seems fine right? Lets use wget to try and import this into our home directory. wget https://github.com/BayLab/MarineGenomicsData/blob/main/week10.tar.gz Now lets try and untar it tar -xvzf week10.tar.gz This prints an error message: gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error is not recoverable: exiting now What seems to have gone wrong. We get a clue if we use less to view the file (you normally wouldnt use less to view to a tar.gz file, but in this case it will tell us something useful). Useful tip: less does work on .gz files! less week10.tar.gz You will see something like this: &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot; data-color-mode=&quot;auto&quot; data-light-theme=&quot;light&quot; data-dark-theme=&quot;dark&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;link rel=&quot;dns-prefetch&quot; href=&quot;https://github.githubassets.com&quot;&gt; &lt;link rel=&quot;dns-prefetch&quot; href=&quot;https://avatars.githubusercontent.com&quot;&gt; &lt;link rel=&quot;dns-prefetch&quot; href=&quot;https://github-cloud.s3.amazonaws.com&quot;&gt; &lt;link rel=&quot;dns-prefetch&quot; href=&quot;https://user-images.githubusercontent.com/&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://github.githubassets.com&quot; crossorigin&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;https://avatars.githubusercontent.com&quot;&gt; &lt;link crossorigin=&quot;anonymous&quot; media=&quot;all&quot; integrity=&quot;sha512-Xvl7qd6ZFq6aBrViMpY+7UKRL79QzxxYG1kyELGe/sH4sV3eCks8DDXxa3WolACcKPac42eqrfe6m0jazyAIPQ==&quot; rel=&quot;stylesheet&quot; href=&quot;https://github.githubassets.com/assets/frameworks-5ef97ba9de9916ae9a06b56232963eed.css&quot; /&gt; &lt;link crossorigin=&quot;anonymous&quot; media=&quot;all&quot; integrity=&quot;sha512-24GJDHWJro3USSMV5JFy5QbE8eCNYG61UucNp7vJMTaeJMrBy6FLiLFgX9jXnWlddv2VRu/rTLIkxzuRDF9ZVA==&quot; rel=&quot;stylesheet&quot; href=&quot;https://github.githubassets.com/assets/colors-v2-db81890c7589ae8dd4492315e49172e5.css&quot; /&gt; &lt;link crossorigin=&quot;anonymous&quot; media=&quot;all&quot; integrity=&quot;sha512-rcBopHrwspQORpXVLihZMP22sFwuIo3fL1DyFo5aXwWnV5FzV/nlAGnX/36fI9GQVc2VN7MiIT34RMCwq8jemg==&quot; rel=&quot;stylesheet&quot; href=&quot;https://github.githubassets.com/assets/behaviors-adc068a47af0b2940e4695d52e285930.css&quot; /&gt; This is not what should be in the file. Were seeing html coding like what you would see for a website. The reason for this is because the link we got from github was to the html page showing the file, not the location of the actual file itself. Lets remove this week10.tar.gz file rm week10.tar.gz So all of the repositories are encoded as html files to make the github website, we need to get the actual file path of the file itself to use wget. You can do this by copying the raw file path, which you will find by clicking view raw or by using control-c or right clicking and selecting copy link address (for pc users) the download tab. Week3_rawgitpath.jpg Lets try this again with our new link. It should say the word raw somewhere in the file path wget https://github.com/BayLab/MarineGenomicsData/raw/main/week10.tar.gz Now untar it tar -xzvf week10.tar.gz You should see something like this, which tells us that it worked!! MarineGenomicsData/Week10/ MarineGenomicsData/Week10/candidate_fastas.fa You can remove this as we wont be using it later in the course rm -r MarineGenomicsData So we need to find that raw file path to use wget on any single file that we want in git hub. 4.5 Final bash wrap-up Weve covered a lot of ground so far in the last 2 and a half weeks! Its a good time to review the commands weve learned and the skills were starting to develop. In week 1, we showed you: how to access jetstream a cloud computing resourse how to navigate the terminal with bash/UNIX commands such as ls, cd,mv,mkdir, andcp` the differnce between full and relative file paths: ++ full path example: /home/margeno/MarineGenomics/week2/README.txt ++ relative path from the MarineGenomics directroy: week2/README.txt how to use Tab to autofill commands and file paths the man command to see full parameters for bash commands In week 2, we covered: how to view files using less, cat, head, tail how to view and modify file and directory permissions using chmod how to use wildcards like * to view directory contents to oh so cautiously use rm to permanently delete a file use grep to search a file and &gt;&gt; to append search results to a new file how to write a script using a text editor nano in our case executing a script from a saved file with bash or by making it an executable program with chmod writing for loops Finally in week3, we learned how to use Awk to edit the contents of a file how to move and download data At this point if you havent already its a good time to make a cheat sheet of the commands weve learned to keep by your computer so you can reference them at anytime. There are several very good bash/Unix cheat sheets available online. Here are links to a few of them: https://cheatography.com/gregcheater/cheat-sheets/bash/ https://cheatography.com/davechild/cheat-sheets/linux-command-line/ https://www.loggly.com/blog/the-essential-cheat-sheet-for-linux-admins/?utm_source=LinkInPDF&amp;utm_medium=social-media&amp;utm_campaign=SocialPush "],["week-4--what-is-a-genetic-variant.html", "5 Week 4- What is a Genetic Variant? 5.1 To get started lets download the data and install a few programs 5.2 Raw read quality control 5.3 Trimming to remove adapters 5.4 Building an index of our genome 5.5 Map reads to the genome 5.6 sam to bam file conversion 5.7 Genotype likelihoods", " 5 Week 4- What is a Genetic Variant? Youll find the lecture discussing the definition and identification of genetic variation here Well be working on the terminal in jetstream for the entirety of this lesson. Here is the road map of our actions for this class (as shown in lecture): mapping variants 5.1 To get started lets download the data and install a few programs Download the data from the MarineGenomicsData repository on git hub. Well be working in the week4_semester.tar.gz file The data for this week comes from an excellent marine genomics study on sea cucumber population genetics (Xuereb et al. 2018). I have taken a sub-sample of the reference genome used in the paper (from the closely related sea cucumber Parastichopus parvimensis). I have also sub-sampled raw reads from 5 individuals of the study species Parastichopus californicus from the paper. wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week4_semester.tar.gz Uncompress the file using tar: tar -xzvf week4_semester.tar.gz Next we need to install a few programs that will allow us to do what we need to do. This will all take a few minutes! The programs that we are installing: samtools: allows us to filter and view our mapped data bowtie2: to map our reads to the reference genome cutadapt: will trim adaptor sequences from the reads fastqc: used to view the quality of the read files sudo apt-get -y update &amp;&amp; \\ sudo apt-get -y install samtools bowtie2 cutadapt fastqc And one more program that well install separately. This is angsd which we will use to find variants in our data. The first command navigates you to your home directory. cd git clone --recursive https://github.com/samtools/htslib.git git clone https://github.com/ANGSD/angsd.git cd htslib;make;cd ../angsd ;make HTSSRC=../htslib Now were ready to get going. The first thing well do is have a look at our data and directories to make sure we know where everything is. $ ls Change directories to the one that has the data MarineGenomics/week4. If you ls into this directory you should see 6 files with a .fastq.gz extension and 1 tiny genome file with a .fna.gz extension. 5.2 Raw read quality control For the sake of time weve make this an optional activity for you to pursue on your own time Raw read quality control Next lets use the program fastqc to check the quality of our data files $ fastqc SRR6805880.tiny.fastq.gz Readout will say: Started analysis for SRR6805880.tiny.fastq.gz Analysis complete for SRR6805880.tiny.fastq.gz Lets look to see that it worked $ ls Ppar_tinygenome.fna.gz SRR6805880.tiny_fastqc.zip SRR6805883.tiny.fastq.gz SRR6805880.tiny.fastq.gz SRR6805881.tiny.fastq.gz SRR6805884.tiny.fastq.gz SRR6805880.tiny_fastqc.html SRR6805882.tiny.fastq.gz SRR6805885.tiny.fastq.gz Looks good! Fastqc generated two outputs for us, a .html and a .zip directory Lets run fastqc on the remaining files, and then well take a look at the output. You may have noticed fastqc just used the same file name to produce our output with different extensions. We can take advantage of that by running fastqc on all our datafiles with the wildcard *. $ fastqc SRR680588* Youll see you initially get an error message because fastqc doesnt see the .fastq file extension on some of our files. It simply skips these and moves on the the next file. To view the output of fastqc, well minimize our terminal and look at our Home folder on our jetstream desktop. This is the same home directory that weve been working in through the terminal. Go to the directory where you were running fastqc and find an .html file. Double click it and it should open a web browser with the output data. Well go over how to interpret this file in class. 5.3 Trimming to remove adapters There are many programs that can be used to trim sequence files. Well use the same paper that was used in the Xuereb et al. 2018 paper here. Cutadapt is relatively easy to run with the code below, once we have identified our adaptor sequence and takes the general form below. $ cutadapt -g SEQUENCETOTRIM -o name_of_input_file name_of_output_file Lets do this on one of our files to test it out. cutadapt -g TGCAG SRR6805880.tiny.fastq.gz -o SRR6805880.tiny_trimmed.fastq.gz This works for a single file, but if we want to do it for all our read files we need to either do them all individually (slow and error prone) or use a for loop. for filename in *.tiny.fastq.gz do base=$(basename $filename .tiny.fastq.gz) echo ${base} cutadapt -g TGCAG ${base}.tiny.fastq.gz -o ${base}.tiny_trimmed.fastq.gz done Yay! You should see a little report for each of these files that showing how many reads were trimmed and some other info (how long are the reads, etc) You can check if the trimmed files are there with: ls *trimmed* Our reads are now ready to be mapped to the genome. 5.4 Building an index of our genome First we have to index our genome. Well do that with the bowtie2-build command. This will generate a lot of files that describe different aspects of our genome We give bowtie2-build two things, the name of our genome, and a general name to label the output files. I always keep the name of the output files the same as the original genome file (without the .fna.gz extension) to avoid confusion (whats this file for?). bowtie2-build Ppar_tinygenome.fna.gz Ppar_tinygenome This should produce several output files with extensions including: .bt2 and rev.1.bt2 etc (six files in total) 5.5 Map reads to the genome Lets map those reads using a for loop for filename in *.tiny_trimmed.fastq.gz do base=$(basename $filename .tiny_trimmed.fastq.gz) echo ${base} bowtie2 -x Ppar_tinygenome -U ${base}.tiny_trimmed.fastq.gz -S ${base}.sam done You should see a bunch of text telling you all about how well our reads mapped to the genome. For this example were getting a low percentage (20-30%) because of how the genome and reads were subset for this exercise. The full genome and full read files have a much higher mapping rate (70-80%) than our subset. Youll also notice that we have made a bunch of .sam files. .sam stands for Sequence Alignment Map file. Lets use less to look at one of these files using less There are several columns of data in a sam file 5.6 sam to bam file conversion The next step is to convert our sam file to a bam (Binary Alignment Map file). This gets our file ready to be read by angsd the program were going to use to call SNPs. for filename in *.sam do base=$(basename $filename .sam) echo ${base} samtools view -bhS ${base}.sam | samtools sort -o ${base}.bam done 5.7 Genotype likelihoods There are many ways and many programs that call genotypes. The program that we will use calculates genotype likelihoods, which account for uncertainty due to sequencing errors and/or mapping errors and is one of several programs in the package ANGSD. The purpose of this class is not to discuss which program is the best, but to teach you to use some commonly used programs. angsd needs a text file with the .bam file names listed. We can make that by running the command below ls *.bam &gt; bam.filelist Look at the list: cat bam.filelist Run the following code to calculate genotype likelihoods ../../angsd/angsd -bam bam.filelist -GL 1 -out genotype_likelihoods -doMaf 2 -SNP_pval 1e-2 -doMajorMinor 1 This will generate two files, one with a .arg extension, this has a record of the script we ran to generate the output, and a .maf file that will give you the minor allele frequencies and is the main output file. If you see these two files, Yay!! We did it! 5.7.1 Suggested Exercises For our coding session you can re-run through the above code as it is written. You can also do the below suggestions to extend or modify what we did in Tuesdays class. A possible answer is located beneath each activity, but its possible you will correctly perform the suggestion in a different way. map the untrimmed files to the genome. How do the alignments compare? Solution As a for loop: for filename in *tiny.fastq.gz; do base=$(basename $filename .tiny.fastq.gz); echo=${base}; bowtie2 -x Ppar_tinygenome -U ${base}.tiny.fastq.gz -S ${base}.nottrimmed.sam; done You should see something that by trimming the adapters off we get a higher overall mapping rate: results Run the mapping for loop as a shell script using bash (i.e., store the for loop in a text editor (NANOs or other) and execute the .sh script with bash) Solution this can be done by copying and pasting the for loop in a text editor that you save as for example map_samples_bowtie2.sh. This script is then executed by bash map_samples_bowtie2.sh use cutadapt to trim the sequences to 70 bp like they did in the Xuereb et al. 2018 paper. Write the output of cutadapt to an .70bp.trimmed.fastq.gz and then map these 70bp, trimmed reads to the genome. How do they compare to our .trimmed reads? Solution to find the parameter for maximum read length in cutadapt: cutadapt - help There are a few ways to do this. cutadapt -g TGCAG ${base}.tiny.fastq.gz -u 70 -o ${base}.tiny_70bp_trimmed.fastq.gz change the parameters of the angsd genotype likelihoods command. How many more/less SNPs do we recover if we lower or raise the SNP p-value? To see what the other parameters do run `../../angsd/angsd -h Solution If we remove the -SNP_pval command entirely we get ~72000 sites retained! Wow! That seems like a lot given our ~20% maping rate. If you instead increase the p-value threshold to 1e-3 we find 3 SNPs. Run fastqc on our .trimmed reads and compare the html with the untrimmed files. Solution We should no longer see the red error flag for the per base sequence quality or base pairs conten. code: fastqc *trimmed.fastq.gz "],["week-5--the-r-environment.html", "6 Week 5- The R environment 6.1 Lesson 1: Orientation to R 6.2 Manipulating a vector object 6.3 Operations act on each element of a vector: 6.4 Operations can also work with two vectors: 6.5 A few tips below for working with objects: 6.6 EXERCISE 1.1 6.7 1.2 Characterizing a dataframe 6.8 How to access parts of the data: 6.9 Data Manipulation 6.10 EXERCISE 1.2 6.11 1.3 Subsetting datasets &amp; logicals 6.12 EXERCISE 1.3 indexing by logical statements", " 6 Week 5- The R environment R icon This lesson is modified from materials of the STEMinist_R lessons produced by several UC Davis graduate student and which can be found here. The lessons were shortened here to fit into two sessions 75 minute sessions. These materials are evenly divided between live coding examples performed by the instructor and exercises performed by the students. This class will take place with students typing directly into an R script for the exercises all of which can be found in the Week 5 semester file here You can download the R files for this week via wget in the terminal with the following link: wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week5_semester.tar.gz this is a commpressed file which can be uncompressed via: tar -xzvf week5_semester.tar.gz You can now open R and load in the R_Day_1_Lesson.R file. This is the script that we will work out of for the rest of the week. You can see it contains many commented sections that begin with a #. This allows you to add comments to your code, explaining what you are doing for each line of code. Commenting code is very important! It explains to someone else what your code does, and can even be useful when you revisit your own code after a few weeks/months/years. Be nice to your future self, comment your code. The next section contains the commented out code and the script that is run in R in a format that is more easily readable on a website. 6.1 Lesson 1: Orientation to R R can be used for basic arithmetic: 5+10+23 #&gt; [1] 38 It can also store values in variables: You can assign an object using an assignment operator &lt;- or =. number&lt;-10 numbers&lt;-c(10, 11, 12, 14, 16) You can see your assigned object by typing the name you gave it. number #&gt; [1] 10 numbers #&gt; [1] 10 11 12 14 16 Objects can be numbers or characters: cat&lt;-&quot;meow&quot; dog&lt;-&quot;woof&quot; We can use colons to get sequences of numbers: n&lt;-1:100 Vectors can also include characters (in quotes): c()=concatenate, aka link things together! animals&lt;-c(&quot;woof&quot;, &quot;meow&quot;, &quot;hiss&quot;, &quot;baa&quot;) 6.2 Manipulating a vector object We can get summaries of vectors with summary() summary(n) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1.00 25.75 50.50 50.50 75.25 100.00 We can see how long a vector is with length() length(n) #&gt; [1] 100 You can use square brackets [] to get parts of vectors. n[50] #&gt; [1] 50 6.3 Operations act on each element of a vector: # +2 numbers+2 #&gt; [1] 12 13 14 16 18 # *2 numbers*2 #&gt; [1] 20 22 24 28 32 # mean mean(numbers) #&gt; [1] 12.6 # ^2 numbers^2 #&gt; [1] 100 121 144 196 256 # sum sum(numbers) #&gt; [1] 63 6.4 Operations can also work with two vectors: #define a new object y y&lt;-numbers*2 # n + y numbers + y #&gt; [1] 30 33 36 42 48 # n * y numbers * y #&gt; [1] 200 242 288 392 512 6.5 A few tips below for working with objects: We can keep track of what objects R is using, with the functions ls() and objects() ls() #&gt; [1] &quot;a&quot; &quot;animals&quot; &quot;areaCircle&quot; &quot;beag&quot; &quot;beag_allIND&quot; #&gt; [6] &quot;beag_allIND_final&quot; &quot;candidates&quot; &quot;cat&quot; &quot;ChickWeight&quot; &quot;cov&quot; #&gt; [11] &quot;data&quot; &quot;data_to_plot&quot; &quot;df1&quot; &quot;df2&quot; &quot;df3&quot; #&gt; [16] &quot;df4&quot; &quot;dog&quot; &quot;e&quot; &quot;g&quot; &quot;G&quot; #&gt; [21] &quot;gen&quot; &quot;gen_allIND&quot; &quot;geno&quot; &quot;genos&quot; &quot;i&quot; #&gt; [26] &quot;il&quot; &quot;iris&quot; &quot;listy&quot; &quot;livability&quot; &quot;lrt&quot; #&gt; [31] &quot;lrt_filt&quot; &quot;lrt_rando&quot; &quot;meta&quot; &quot;meta.path&quot; &quot;msleep&quot; #&gt; [36] &quot;my.colors&quot; &quot;my.new&quot; &quot;my_colors&quot; &quot;my_list&quot; &quot;n&quot; #&gt; [41] &quot;names&quot; &quot;number&quot; &quot;numbers&quot; &quot;outliers&quot; &quot;pheno_chr6&quot; #&gt; [46] &quot;qval&quot; &quot;rando_filt&quot; &quot;setosa1.petallength&quot; &quot;setosa1.petalwidth&quot; &quot;setosa1area2&quot; #&gt; [51] &quot;squares&quot; &quot;state.abb&quot; &quot;state.area&quot; &quot;state.center&quot; &quot;state.division&quot; #&gt; [56] &quot;state.name&quot; &quot;state.region&quot; &quot;state.x77&quot; &quot;states&quot; &quot;states_standardized&quot; #&gt; [61] &quot;subgen&quot; &quot;submeta&quot; &quot;subpops&quot; &quot;subsleep&quot; &quot;tG&quot; #&gt; [66] &quot;tr_msleep&quot; &quot;vcf.path&quot; &quot;x&quot; &quot;y&quot; objects() #returns the same results as ls() in this case. because we only have objects in our environment. #&gt; [1] &quot;a&quot; &quot;animals&quot; &quot;areaCircle&quot; &quot;beag&quot; &quot;beag_allIND&quot; #&gt; [6] &quot;beag_allIND_final&quot; &quot;candidates&quot; &quot;cat&quot; &quot;ChickWeight&quot; &quot;cov&quot; #&gt; [11] &quot;data&quot; &quot;data_to_plot&quot; &quot;df1&quot; &quot;df2&quot; &quot;df3&quot; #&gt; [16] &quot;df4&quot; &quot;dog&quot; &quot;e&quot; &quot;g&quot; &quot;G&quot; #&gt; [21] &quot;gen&quot; &quot;gen_allIND&quot; &quot;geno&quot; &quot;genos&quot; &quot;i&quot; #&gt; [26] &quot;il&quot; &quot;iris&quot; &quot;listy&quot; &quot;livability&quot; &quot;lrt&quot; #&gt; [31] &quot;lrt_filt&quot; &quot;lrt_rando&quot; &quot;meta&quot; &quot;meta.path&quot; &quot;msleep&quot; #&gt; [36] &quot;my.colors&quot; &quot;my.new&quot; &quot;my_colors&quot; &quot;my_list&quot; &quot;n&quot; #&gt; [41] &quot;names&quot; &quot;number&quot; &quot;numbers&quot; &quot;outliers&quot; &quot;pheno_chr6&quot; #&gt; [46] &quot;qval&quot; &quot;rando_filt&quot; &quot;setosa1.petallength&quot; &quot;setosa1.petalwidth&quot; &quot;setosa1area2&quot; #&gt; [51] &quot;squares&quot; &quot;state.abb&quot; &quot;state.area&quot; &quot;state.center&quot; &quot;state.division&quot; #&gt; [56] &quot;state.name&quot; &quot;state.region&quot; &quot;state.x77&quot; &quot;states&quot; &quot;states_standardized&quot; #&gt; [61] &quot;subgen&quot; &quot;submeta&quot; &quot;subpops&quot; &quot;subsleep&quot; &quot;tG&quot; #&gt; [66] &quot;tr_msleep&quot; &quot;vcf.path&quot; &quot;x&quot; &quot;y&quot; # how to get help for a function; you can also write help() ?ls # you can get rid of objects you don&#39;t want rm(numbers) # and make sure it got rid of them ls() #&gt; [1] &quot;a&quot; &quot;animals&quot; &quot;areaCircle&quot; &quot;beag&quot; &quot;beag_allIND&quot; #&gt; [6] &quot;beag_allIND_final&quot; &quot;candidates&quot; &quot;cat&quot; &quot;ChickWeight&quot; &quot;cov&quot; #&gt; [11] &quot;data&quot; &quot;data_to_plot&quot; &quot;df1&quot; &quot;df2&quot; &quot;df3&quot; #&gt; [16] &quot;df4&quot; &quot;dog&quot; &quot;e&quot; &quot;g&quot; &quot;G&quot; #&gt; [21] &quot;gen&quot; &quot;gen_allIND&quot; &quot;geno&quot; &quot;genos&quot; &quot;i&quot; #&gt; [26] &quot;il&quot; &quot;iris&quot; &quot;listy&quot; &quot;livability&quot; &quot;lrt&quot; #&gt; [31] &quot;lrt_filt&quot; &quot;lrt_rando&quot; &quot;meta&quot; &quot;meta.path&quot; &quot;msleep&quot; #&gt; [36] &quot;my.colors&quot; &quot;my.new&quot; &quot;my_colors&quot; &quot;my_list&quot; &quot;n&quot; #&gt; [41] &quot;names&quot; &quot;number&quot; &quot;outliers&quot; &quot;pheno_chr6&quot; &quot;qval&quot; #&gt; [46] &quot;rando_filt&quot; &quot;setosa1.petallength&quot; &quot;setosa1.petalwidth&quot; &quot;setosa1area2&quot; &quot;squares&quot; #&gt; [51] &quot;state.abb&quot; &quot;state.area&quot; &quot;state.center&quot; &quot;state.division&quot; &quot;state.name&quot; #&gt; [56] &quot;state.region&quot; &quot;state.x77&quot; &quot;states&quot; &quot;states_standardized&quot; &quot;subgen&quot; #&gt; [61] &quot;submeta&quot; &quot;subpops&quot; &quot;subsleep&quot; &quot;tG&quot; &quot;tr_msleep&quot; #&gt; [66] &quot;vcf.path&quot; &quot;x&quot; &quot;y&quot; 6.6 EXERCISE 1.1 Open Rstudio and perform an arithmetic calculation in the command line. Solution #this can be whatever you decide to do! 5*134 #&gt; [1] 670 Create a numeric vector in the command line containing: the numbers 2, 9, 3, 8, and 3 and assign this vector to a global variable x. Perform arithmetic with x. Convince yourself R works as a calculator, and knows order of operations. Multiply x by 10, and save the result as a new object named y Calculate the difference in the sum of the x vector and the sum of the y vector Solution x &lt;- c(2, 9, 3, 8, 3) x * 20 #&gt; [1] 40 180 60 160 60 x + 4 * 24 #&gt; [1] 98 105 99 104 99 y &lt;- x * 10 sum(x) - sum(y) #&gt; [1] -225 Call the help files for the functions ls() and rm() What are the arguments for the ls() function? What does the sorted argument do? Solution ?ls #From the help file: sorted is a logical indicating if the resulting character should be sorted alphabetically. Note that this is part of ls() may take most of the time. 6.7 1.2 Characterizing a dataframe Well now move from working with objects and vectors to working with dataframes: Here are a few useful functions: install.packages() library() data() str() dim() colnames() and rownames() class() as.factor() as.numeric() unique() t() max(), min(), mean() and summary() Were going to use data on sleep patterns in mammals. This requires installing a package (ggplot2) and loading the data Install the package ggplot2. This only has to be done once and after installation we should then comment out the command to install the package with a #. #install.packages(&quot;ggplot2&quot;) #load the package library (ggplot2) Load the data (its called msleep). data(&quot;msleep&quot;) There are many functions in R that allow us to get an idea of what the data looks like. For example, what are its dimensions (how many rows and columns)? # head() -look at the beginning of the data file # tail() -look at the end of the data file head(msleep) #&gt; # A tibble: 6 x 11 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Cheetah Acinonyx carni Carnivora lc 12.1 NA NA 11.9 NA 50 #&gt; 2 Owl monkey Aotus omni Primates &lt;NA&gt; 17 1.8 NA 7 0.0155 0.48 #&gt; 3 Mountain beaver Aplodontia herbi Rodentia nt 14.4 2.4 NA 9.6 NA 1.35 #&gt; 4 Greater short-tailed shrew Blarina omni Soricomorpha lc 14.9 2.3 0.133 9.1 0.00029 0.019 #&gt; 5 Cow Bos herbi Artiodactyla domesticated 4 0.7 0.667 20 0.423 600 #&gt; 6 Three-toed sloth Bradypus herbi Pilosa &lt;NA&gt; 14.4 2.2 0.767 9.6 NA 3.85 tail(msleep) #&gt; # A tibble: 6 x 11 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Tenrec Tenrec omni Afrosoricida &lt;NA&gt; 15.6 2.3 NA 8.4 0.0026 0.9 #&gt; 2 Tree shrew Tupaia omni Scandentia &lt;NA&gt; 8.9 2.6 0.233 15.1 0.0025 0.104 #&gt; 3 Bottle-nosed dolphin Tursiops carni Cetacea &lt;NA&gt; 5.2 NA NA 18.8 NA 173. #&gt; 4 Genet Genetta carni Carnivora &lt;NA&gt; 6.3 1.3 NA 17.7 0.0175 2 #&gt; 5 Arctic fox Vulpes carni Carnivora &lt;NA&gt; 12.5 NA NA 11.5 0.0445 3.38 #&gt; 6 Red fox Vulpes carni Carnivora &lt;NA&gt; 9.8 2.4 0.35 14.2 0.0504 4.23 # str() str(msleep) #&gt; tibble [83 x 11] (S3: tbl_df/tbl/data.frame) #&gt; $ name : chr [1:83] &quot;Cheetah&quot; &quot;Owl monkey&quot; &quot;Mountain beaver&quot; &quot;Greater short-tailed shrew&quot; ... #&gt; $ genus : chr [1:83] &quot;Acinonyx&quot; &quot;Aotus&quot; &quot;Aplodontia&quot; &quot;Blarina&quot; ... #&gt; $ vore : chr [1:83] &quot;carni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;omni&quot; ... #&gt; $ order : chr [1:83] &quot;Carnivora&quot; &quot;Primates&quot; &quot;Rodentia&quot; &quot;Soricomorpha&quot; ... #&gt; $ conservation: chr [1:83] &quot;lc&quot; NA &quot;nt&quot; &quot;lc&quot; ... #&gt; $ sleep_total : num [1:83] 12.1 17 14.4 14.9 4 14.4 8.7 7 10.1 3 ... #&gt; $ sleep_rem : num [1:83] NA 1.8 2.4 2.3 0.7 2.2 1.4 NA 2.9 NA ... #&gt; $ sleep_cycle : num [1:83] NA NA NA 0.133 0.667 ... #&gt; $ awake : num [1:83] 11.9 7 9.6 9.1 20 9.6 15.3 17 13.9 21 ... #&gt; $ brainwt : num [1:83] NA 0.0155 NA 0.00029 0.423 NA NA NA 0.07 0.0982 ... #&gt; $ bodywt : num [1:83] 50 0.48 1.35 0.019 600 ... dim(), ncol(), nrow()- dimensions, number of columns, number of rows colnames(), rownames() - column names, row names Rstudio also allows us to just look into the data file with View() 6.8 How to access parts of the data: We can also look at a single column at a time. There are three ways to access this: $, [,#] or [,a]. Quick Tip: Think about rc cola or remote control car to remember that [5,] means fifth row and [,5] means fifth column! Each way has its own advantages: msleep[,3] #&gt; # A tibble: 83 x 1 #&gt; vore #&gt; &lt;chr&gt; #&gt; 1 carni #&gt; 2 omni #&gt; 3 herbi #&gt; 4 omni #&gt; 5 herbi #&gt; 6 herbi #&gt; 7 carni #&gt; 8 &lt;NA&gt; #&gt; 9 carni #&gt; 10 herbi #&gt; # ... with 73 more rows msleep[, &quot;vore&quot;] #&gt; # A tibble: 83 x 1 #&gt; vore #&gt; &lt;chr&gt; #&gt; 1 carni #&gt; 2 omni #&gt; 3 herbi #&gt; 4 omni #&gt; 5 herbi #&gt; 6 herbi #&gt; 7 carni #&gt; 8 &lt;NA&gt; #&gt; 9 carni #&gt; 10 herbi #&gt; # ... with 73 more rows msleep$vore #&gt; [1] &quot;carni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;omni&quot; &quot;herbi&quot; &quot;herbi&quot; &quot;carni&quot; NA &quot;carni&quot; &quot;herbi&quot; &quot;herbi&quot; &quot;herbi&quot; #&gt; [13] &quot;omni&quot; &quot;herbi&quot; &quot;omni&quot; &quot;omni&quot; &quot;omni&quot; &quot;carni&quot; &quot;herbi&quot; &quot;omni&quot; &quot;herbi&quot; &quot;insecti&quot; &quot;herbi&quot; &quot;herbi&quot; #&gt; [25] &quot;omni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;carni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;carni&quot; &quot;carni&quot; &quot;herbi&quot; &quot;omni&quot; &quot;herbi&quot; &quot;herbi&quot; #&gt; [37] &quot;carni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;herbi&quot; &quot;herbi&quot; &quot;herbi&quot; &quot;insecti&quot; &quot;herbi&quot; &quot;carni&quot; &quot;herbi&quot; &quot;carni&quot; &quot;herbi&quot; #&gt; [49] &quot;herbi&quot; &quot;omni&quot; &quot;carni&quot; &quot;carni&quot; &quot;carni&quot; &quot;omni&quot; NA &quot;omni&quot; NA NA &quot;carni&quot; &quot;carni&quot; #&gt; [61] &quot;herbi&quot; &quot;insecti&quot; NA &quot;herbi&quot; &quot;omni&quot; &quot;omni&quot; &quot;insecti&quot; &quot;herbi&quot; NA &quot;herbi&quot; &quot;herbi&quot; &quot;herbi&quot; #&gt; [73] NA &quot;omni&quot; &quot;insecti&quot; &quot;herbi&quot; &quot;herbi&quot; &quot;omni&quot; &quot;omni&quot; &quot;carni&quot; &quot;carni&quot; &quot;carni&quot; &quot;carni&quot; Sometimes it is useful to know what class() the column is: class(msleep$vore) #&gt; [1] &quot;character&quot; class(msleep$sleep_total) #&gt; [1] &quot;numeric&quot; We can also look at a single row at a time. There are two ways to access this: 1. by indicating the row number in square brackets next to the name of the dataframe name[#,] and by calling the actual name of the row (if your rows have names) name[\"a\",]. msleep[43,] #&gt; # A tibble: 1 x 11 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Little brown bat Myotis insecti Chiroptera &lt;NA&gt; 19.9 2 0.2 4.1 0.00025 0.01 msleep[msleep$name == &quot;Mountain beaver&quot;,] #&gt; # A tibble: 1 x 11 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mountain beaver Aplodontia herbi Rodentia nt 14.4 2.4 NA 9.6 NA 1.35 We can select more than one row or column at a time: # see two columns msleep[,c(1, 6)] #&gt; # A tibble: 83 x 2 #&gt; name sleep_total #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Cheetah 12.1 #&gt; 2 Owl monkey 17 #&gt; 3 Mountain beaver 14.4 #&gt; 4 Greater short-tailed shrew 14.9 #&gt; 5 Cow 4 #&gt; 6 Three-toed sloth 14.4 #&gt; 7 Northern fur seal 8.7 #&gt; 8 Vesper mouse 7 #&gt; 9 Dog 10.1 #&gt; 10 Roe deer 3 #&gt; # ... with 73 more rows # and make a new data frame from these subsets subsleep&lt;-msleep[,c(1, 6)] But what if we actually care about how many unique things are in a column? # unique() unique(msleep[, &quot;order&quot;]) #&gt; # A tibble: 19 x 1 #&gt; order #&gt; &lt;chr&gt; #&gt; 1 Carnivora #&gt; 2 Primates #&gt; 3 Rodentia #&gt; 4 Soricomorpha #&gt; 5 Artiodactyla #&gt; 6 Pilosa #&gt; 7 Cingulata #&gt; 8 Hyracoidea #&gt; 9 Didelphimorphia #&gt; 10 Proboscidea #&gt; 11 Chiroptera #&gt; 12 Perissodactyla #&gt; 13 Erinaceomorpha #&gt; 14 Cetacea #&gt; 15 Lagomorpha #&gt; 16 Diprotodontia #&gt; 17 Monotremata #&gt; 18 Afrosoricida #&gt; 19 Scandentia # table() table(msleep$order) #&gt; #&gt; Afrosoricida Artiodactyla Carnivora Cetacea Chiroptera Cingulata Didelphimorphia Diprotodontia #&gt; 1 6 12 3 2 2 2 2 #&gt; Erinaceomorpha Hyracoidea Lagomorpha Monotremata Perissodactyla Pilosa Primates Proboscidea #&gt; 2 3 1 1 3 1 12 2 #&gt; Rodentia Scandentia Soricomorpha #&gt; 22 1 5 # levels(), if class is factor (and if not we can make it a factor) levels(as.factor(msleep$order)) #&gt; [1] &quot;Afrosoricida&quot; &quot;Artiodactyla&quot; &quot;Carnivora&quot; &quot;Cetacea&quot; &quot;Chiroptera&quot; &quot;Cingulata&quot; &quot;Didelphimorphia&quot; #&gt; [8] &quot;Diprotodontia&quot; &quot;Erinaceomorpha&quot; &quot;Hyracoidea&quot; &quot;Lagomorpha&quot; &quot;Monotremata&quot; &quot;Perissodactyla&quot; &quot;Pilosa&quot; #&gt; [15] &quot;Primates&quot; &quot;Proboscidea&quot; &quot;Rodentia&quot; &quot;Scandentia&quot; &quot;Soricomorpha&quot; 6.9 Data Manipulation If your data is transposed in a way that isnt useful to you, you can switch it. Note that this often changes the class of each column! In R, each column must have the same type of data: # t() tr_msleep&lt;-t(msleep) str(tr_msleep) #&gt; chr [1:11, 1:83] &quot;Cheetah&quot; &quot;Acinonyx&quot; &quot;carni&quot; &quot;Carnivora&quot; &quot;lc&quot; &quot;12.1&quot; NA NA &quot;11.90&quot; NA &quot; 50.000&quot; &quot;Owl monkey&quot; &quot;Aotus&quot; &quot;omni&quot; ... #&gt; - attr(*, &quot;dimnames&quot;)=List of 2 #&gt; ..$ : chr [1:11] &quot;name&quot; &quot;genus&quot; &quot;vore&quot; &quot;order&quot; ... #&gt; ..$ : NULL Its important to know the class of data if you want to manipulate it. For example, you cant add characters. msleep contains several different types of data. Some common classes are: factors, numeric, integers, characters, logical # class() class(msleep) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # str() str(msleep) #&gt; tibble [83 x 11] (S3: tbl_df/tbl/data.frame) #&gt; $ name : chr [1:83] &quot;Cheetah&quot; &quot;Owl monkey&quot; &quot;Mountain beaver&quot; &quot;Greater short-tailed shrew&quot; ... #&gt; $ genus : chr [1:83] &quot;Acinonyx&quot; &quot;Aotus&quot; &quot;Aplodontia&quot; &quot;Blarina&quot; ... #&gt; $ vore : chr [1:83] &quot;carni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;omni&quot; ... #&gt; $ order : chr [1:83] &quot;Carnivora&quot; &quot;Primates&quot; &quot;Rodentia&quot; &quot;Soricomorpha&quot; ... #&gt; $ conservation: chr [1:83] &quot;lc&quot; NA &quot;nt&quot; &quot;lc&quot; ... #&gt; $ sleep_total : num [1:83] 12.1 17 14.4 14.9 4 14.4 8.7 7 10.1 3 ... #&gt; $ sleep_rem : num [1:83] NA 1.8 2.4 2.3 0.7 2.2 1.4 NA 2.9 NA ... #&gt; $ sleep_cycle : num [1:83] NA NA NA 0.133 0.667 ... #&gt; $ awake : num [1:83] 11.9 7 9.6 9.1 20 9.6 15.3 17 13.9 21 ... #&gt; $ brainwt : num [1:83] NA 0.0155 NA 0.00029 0.423 NA NA NA 0.07 0.0982 ... #&gt; $ bodywt : num [1:83] 50 0.48 1.35 0.019 600 ... Often we want to summarize data. There are many ways of doing this in R: # calculate mean() of a column mean(msleep$sleep_total) #&gt; [1] 10.43373 # max() max(msleep$sleep_total) #&gt; [1] 19.9 # min() min(msleep$sleep_total) #&gt; [1] 1.9 # summary() summary(msleep$sleep_total) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1.90 7.85 10.10 10.43 13.75 19.90 Sometimes, the values we care about arent provided in a data set. When this happens, we can create a new column that contains the values were interested in: # what if what we cared about was our sleep_total/sleep_rem ratio? # add a sleep_total/sleep_rem ratio column to our msleep dataframe with $ msleep$total_rem&lt;-msleep$sleep_total/msleep$sleep_rem # look at our dataframe again. It now contains 12 columns, one of them being the one we just created. head(msleep) #&gt; # A tibble: 6 x 12 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt total_rem #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Cheetah Acinonyx carni Carn~ lc 12.1 NA NA 11.9 NA 50 NA #&gt; 2 Owl monkey Aotus omni Prim~ &lt;NA&gt; 17 1.8 NA 7 0.0155 0.48 9.44 #&gt; 3 Mountain beaver Aplodontia herbi Rode~ nt 14.4 2.4 NA 9.6 NA 1.35 6 #&gt; 4 Greater short-tailed shrew Blarina omni Sori~ lc 14.9 2.3 0.133 9.1 0.00029 0.019 6.48 #&gt; 5 Cow Bos herbi Arti~ domesticated 4 0.7 0.667 20 0.423 600 5.71 #&gt; 6 Three-toed sloth Bradypus herbi Pilo~ &lt;NA&gt; 14.4 2.2 0.767 9.6 NA 3.85 6.55 6.10 EXERCISE 1.2 Reminder of those useful commands: dataframename[row , col], str(), dim(), nrow(), unique(), length(), rownames(), summary(), min(), max(), mean(), range(), levels(), factor(), as.factor(), class(), ncol(), nrow(), table(), sum(), quantile(), var() Well use the built-in iris dataset. the command: data(iris) # this loads the iris dataset. You can view more information about this dataset with help(iris) or ?iris How many rows are in the dataset? Solution data(iris) nrow(iris) #&gt; [1] 150 What are three distinct ways to figure this out? Solution #nrows #str #dim How many species of flowers are in the dataset? Solution levels(iris$Species) #&gt; [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; What class is iris? Solution class(iris) #&gt; [1] &quot;data.frame&quot; How many columns does this data frame have? What are their names? Solution colnames(iris) #&gt; [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; What class did R assign to each column? Solution str(iris) #&gt; &#39;data.frame&#39;: 150 obs. of 5 variables: #&gt; $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #&gt; $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #&gt; $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #&gt; $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... #&gt; $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Assign the first flowers petals width and length to new objects called setosa1.petalwidth and setosa1.petallength Solution setosa1.petalwidth&lt;-iris[1,2] setosa1.petallength&lt;-iris[1,3] Calculate the approximate area of the petal of the first flower, setosa1 (assume petal area can be approximated by a rectangle). Solution #using our premade objects setosa1area2&lt;-setosa1.petalwidth*setosa1.petallength Calculate the petal area of each flower in the iris dataset and assign this to a new column named PetalArea. Solution iris$PetalArea&lt;-iris$Petal.Length*iris$Petal.Width What is the maximum sepal length of the irises? Solution max(iris$Sepal.Length) #&gt; [1] 7.9 What is the average sepal length among all flowers in the dataset? Solution mean(iris$Sepal.Length) #&gt; [1] 5.843333 How about the minimum and median sepal length? Solution min(iris$Sepal.Length) #&gt; [1] 4.3 median(iris$Sepal.Length) #&gt; [1] 5.8 6.11 1.3 Subsetting datasets &amp; logicals A few useful commands: equals ==, does not equal !=, greater than &gt;, less than &lt;, and &amp;, and a pipe which can also indicate and |. Reminder there are two assignment operators in R &lt;- and a single equals sign =. The one you use really depends on how you learned to use R, and are otherwise equivalent. Logical conditions vs. assignment operators: Logical values of TRUE and FALSE are special in R. What class is a logical value? TRUE #&gt; [1] TRUE FALSE #&gt; [1] FALSE # what class is a logical value? class(TRUE) #&gt; [1] &quot;logical&quot; Logical values are stored as 0 for FALSE and 1 for TRUE. Which means you can do math with them! TRUE + 1 #&gt; [1] 2 FALSE + 1 #&gt; [1] 1 sum(c(TRUE,TRUE,FALSE,FALSE)) #&gt; [1] 2 !TRUE #&gt; [1] FALSE !c(TRUE,TRUE,FALSE,FALSE) #&gt; [1] FALSE FALSE TRUE TRUE Logicals will be the output of various tests: 1 == 1 #&gt; [1] TRUE 1 == 2 #&gt; [1] FALSE # does not equal 1 != 1 #&gt; [1] FALSE 1 != 2 #&gt; [1] TRUE # greater than 1 &gt; 1 #&gt; [1] FALSE 1 &gt;= 1 #&gt; [1] TRUE # less than 1 &lt; 3 #&gt; [1] TRUE # combining logical conditions with and (&amp;), or(|) 1 == 1 &amp; 2 == 2 #&gt; [1] TRUE 1 == 1 &amp; 1 == 2 #&gt; [1] FALSE 1 == 1 | 1 == 2 #&gt; [1] TRUE # we can take the opposite of a logical by using ! !TRUE #&gt; [1] FALSE This is very useful because we can use logicals to query a data frame or vector. # Which numbers in 1:10 are greater than 3? 1:10 &gt; 3 #&gt; [1] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # How many numbers in 1:10 are greater than 3? sum(1:10 &gt; 3) #&gt; [1] 7 # in our msleep data frame, which species have total sleep greater than 18 hours? # reload the msleep data with library(ggplot2) and data(msleep) if you need to msleep[,&quot;sleep_total&quot;]&gt;18 #&gt; sleep_total #&gt; [1,] FALSE #&gt; [2,] FALSE #&gt; [3,] FALSE #&gt; [4,] FALSE #&gt; [5,] FALSE #&gt; [6,] FALSE #&gt; [7,] FALSE #&gt; [8,] FALSE #&gt; [9,] FALSE #&gt; [10,] FALSE #&gt; [11,] FALSE #&gt; [12,] FALSE #&gt; [13,] FALSE #&gt; [14,] FALSE #&gt; [15,] FALSE #&gt; [16,] FALSE #&gt; [17,] FALSE #&gt; [18,] FALSE #&gt; [19,] FALSE #&gt; [20,] FALSE #&gt; [21,] FALSE #&gt; [22,] TRUE #&gt; [23,] FALSE #&gt; [24,] FALSE #&gt; [25,] FALSE #&gt; [26,] FALSE #&gt; [27,] FALSE #&gt; [28,] FALSE #&gt; [29,] FALSE #&gt; [30,] FALSE #&gt; [31,] FALSE #&gt; [32,] FALSE #&gt; [33,] FALSE #&gt; [34,] FALSE #&gt; [35,] FALSE #&gt; [36,] FALSE #&gt; [37,] TRUE #&gt; [38,] FALSE #&gt; [39,] FALSE #&gt; [40,] FALSE #&gt; [41,] FALSE #&gt; [42,] FALSE #&gt; [43,] TRUE #&gt; [44,] FALSE #&gt; [45,] FALSE #&gt; [46,] FALSE #&gt; [47,] FALSE #&gt; [48,] FALSE #&gt; [49,] FALSE #&gt; [50,] FALSE #&gt; [51,] FALSE #&gt; [52,] FALSE #&gt; [53,] FALSE #&gt; [54,] FALSE #&gt; [55,] FALSE #&gt; [56,] FALSE #&gt; [57,] FALSE #&gt; [58,] FALSE #&gt; [59,] FALSE #&gt; [60,] FALSE #&gt; [61,] FALSE #&gt; [62,] TRUE #&gt; [63,] FALSE #&gt; [64,] FALSE #&gt; [65,] FALSE #&gt; [66,] FALSE #&gt; [67,] FALSE #&gt; [68,] FALSE #&gt; [69,] FALSE #&gt; [70,] FALSE #&gt; [71,] FALSE #&gt; [72,] FALSE #&gt; [73,] FALSE #&gt; [74,] FALSE #&gt; [75,] FALSE #&gt; [76,] FALSE #&gt; [77,] FALSE #&gt; [78,] FALSE #&gt; [79,] FALSE #&gt; [80,] FALSE #&gt; [81,] FALSE #&gt; [82,] FALSE #&gt; [83,] FALSE # Using which() to identify which rows match the logical values (TRUE) and length to count how many species there are which(msleep[,&quot;sleep_total&quot;]&gt;18) #22 37 43 62 --&gt; the rows that contain organisms that sleep more than 18 hrs #&gt; [1] 22 37 43 62 length(which(msleep[,&quot;sleep_total&quot;]&gt;18)) #4 --&gt; number of species that sleep more than 18 hrs #&gt; [1] 4 # which four species are these? msleep[which(msleep[,&quot;sleep_total&quot;]&gt;18),] #&gt; # A tibble: 4 x 12 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt total_rem #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Big brown bat Eptesicus insecti Chiroptera lc 19.7 3.9 0.117 4.3 3 e-4 0.023 5.05 #&gt; 2 Thick-tailed opposum Lutreolina carni Didelphimorphia lc 19.4 6.6 NA 4.6 NA 0.37 2.94 #&gt; 3 Little brown bat Myotis insecti Chiroptera &lt;NA&gt; 19.9 2 0.2 4.1 2.5e-4 0.01 9.95 #&gt; 4 Giant armadillo Priodontes insecti Cingulata en 18.1 6.1 NA 5.9 8.1e-2 60 2.97 # what if we only want to see the bats that sleep more than 18 hours per 24 hour period? msleep[which(msleep[,&quot;sleep_total&quot;]&gt;18 &amp; msleep[,&quot;order&quot;] == &quot;Chiroptera&quot;),] #&gt; # A tibble: 2 x 12 #&gt; name genus vore order conservation sleep_total sleep_rem sleep_cycle awake brainwt bodywt total_rem #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Big brown bat Eptesicus insecti Chiroptera lc 19.7 3.9 0.117 4.3 0.0003 0.023 5.05 #&gt; 2 Little brown bat Myotis insecti Chiroptera &lt;NA&gt; 19.9 2 0.2 4.1 0.00025 0.01 9.95 6.12 EXERCISE 1.3 indexing by logical statements A few useful commands: ==, !=, &gt;, &lt;, &amp;, |, sum(), which(), table(), ! 1. Create your own logical vector with three TRUEs and three FALSEs Solution a = c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE) a ## let&#39;s print to screen and make sure it is stored in this variable #&gt; [1] TRUE TRUE FALSE FALSE TRUE FALSE Produce a vector of the index number of the TRUE values Solution which(a) ## which gives you the index of TRUE values automatically #&gt; [1] 1 2 5 which(a == TRUE) ## but sometimes it&#39;s reassuring to state exactly what you&#39;re doing #&gt; [1] 1 2 5 Produce a second vector which indexes the numbers of the falses Solution which(!a) #&gt; [1] 3 4 6 which(a == FALSE) #&gt; [1] 3 4 6 Go back to the iris dataset, which can be loaded with data(iris) 4. How many irises have sepals less than 5.5 cm? Solution data(iris) ## this reloads the data set in case you&#39;ve closed R since using iris sum(iris[,&#39;Sepal.Length&#39;]&lt;5.5) ## remember TRUE&#39;s are 1 and FALSE&#39;s are 0 #&gt; [1] 52 length(which(iris[,&#39;Sepal.Length&#39;]&lt;5.5)) ## here, which() will only return the index of TRUE values, so we&#39;re counting how many there are #&gt; [1] 52 Which iris individual has the largest petal length? What is the width of its petal? Solution max(iris[,&#39;Petal.Length&#39;]) ## this gives us the length of the longest petal #&gt; [1] 6.9 which(iris[,&#39;Petal.Length&#39;] == max(iris[,&#39;Petal.Length&#39;])) ## this gives us the index of the individual with the longest petal #&gt; [1] 119 iris[,&#39;Petal.Width&#39;][which(iris[,&#39;Petal.Length&#39;] == max(iris[,&#39;Petal.Length&#39;]))] ## now we&#39;re subsetting the Petal.Width column by the index of the individual with the longest petal #&gt; [1] 2.3 ## another way to do this would be to use the index of the individual with the longest petal to pick rows, and the Petal.Width name to pick columns and subset the entire data frame iris[which(iris[,&#39;Petal.Length&#39;] == max(iris[,&#39;Petal.Length&#39;])) , &#39;Petal.Width&#39;] #&gt; [1] 2.3 How many of the irises are in this dataset belong to the species versicolor? Solution sum(iris[,&#39;Species&#39;]==&#39;versicolor&#39;) #&gt; [1] 50 table(iris[,&#39;Species&#39;]) ## this gets us all three species #&gt; #&gt; setosa versicolor virginica #&gt; 50 50 50 How many irises have petals longer than 6cm? Solution sum(iris[,&#39;Petal.Length&#39;] &gt; 6) #&gt; [1] 9 Create a vector of species name for each iris with sepals longer than 6cm. Solution iris[,&#39;Species&#39;][iris[,&#39;Sepal.Length&#39;]&gt;6] #&gt; [1] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor #&gt; [12] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor virginica virginica #&gt; [23] virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica #&gt; [34] virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica #&gt; [45] virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica #&gt; [56] virginica virginica virginica virginica virginica virginica #&gt; Levels: setosa versicolor virginica iris[iris[,&#39;Sepal.Length&#39;]&gt;6, &#39;Species&#39;] ## alternatively, we can put the logical vector in the row part, and Species in the column part, to get a vector back #&gt; [1] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor #&gt; [12] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor virginica virginica #&gt; [23] virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica #&gt; [34] virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica #&gt; [45] virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica virginica #&gt; [56] virginica virginica virginica virginica virginica virginica #&gt; Levels: setosa versicolor virginica How many irises have sepals shorter than 5cm, but wider than 3cm? Solution sum( iris[,&#39;Sepal.Length&#39;] &lt; 5 &amp; iris[,&#39;Sepal.Width&#39;] &gt; 3 ) #&gt; [1] 13 How many irises have petals narrower than 0.2cm or shorter than 1.5cm? Solution sum( iris[,&#39;Petal.Width&#39;] &lt; 0.2 | iris[,&#39;Petal.Length&#39;] &lt; 1.5 ) #&gt; [1] 26 What is the average width of setosa iris sepals that are longer than 5cm? Solution mean( iris[,&#39;Sepal.Width&#39;][iris[,&#39;Sepal.Length&#39;] &gt; 5][iris[,&#39;Species&#39;]==&#39;setosa&#39;]) ## convince yourself the second part is a logical vector that subsets iris[,&#39;Sepal.Width&#39;] #&gt; [1] 3.22 mean( iris[iris[,&#39;Sepal.Length&#39;] &gt; 5, &#39;Sepal.Width&#39;][iris[,&#39;Species&#39;]==&#39;setosa&#39;]) ## again, we can alternatively subset using logical vectors in the row position #&gt; [1] 3.22 What is the difference between the longest and shortest petal lengths of the species virginica? Solution max(iris[,&#39;Petal.Length&#39;][iris[,&#39;Species&#39;]==&#39;virginica&#39;]) - min(iris[,&#39;Petal.Length&#39;][iris[,&#39;Species&#39;]==&#39;virginica&#39;]) #&gt; [1] 2.4 What proportion of flowers in the dataset have petals wider than 1cm? Solution sum(iris[,&#39;Petal.Width&#39;] &gt; 1 ) / nrow(iris) ## here, we&#39;re counting up how many are wider than 1 cm, and dividing by the total number of flowers to get a proportion #&gt; [1] 0.62 Create a new column within your dataframe, called sepalCategory, and set all values equal to long Subset short values of this column, and set their values to short (Short sepals are those less than 5.5 cm) How many plants with short sepals are there? How many long? Solution # new column for long iris[,&#39;sepalCategory&#39;] = &#39;long&#39; ## this sets ever entry in the column equal to &#39;long&#39; # new column for short (&lt; 5.5 cm) iris[,&#39;sepalCategory&#39;][iris[,&#39;Sepal.Length&#39;]&lt;5.5] = &#39;short&#39; ## this sets only those entries that match our condition to &#39;short&#39; # how many plants with short sepals are there? How many long? table(iris[,&#39;sepalCategory&#39;]) #&gt; #&gt; long short #&gt; 98 52 "],["week-6--r-continued.html", "7 Week 6- R Continued 7.1 2.1 Plotting 7.2 Scatterplots 7.3 Customizing your plot 7.4 Exercise 2.1 7.5 2.2 plotting with ggplot2 7.6 Exercise 2.2 Plotting in ggplot2", " 7 Week 6- R Continued R icon Were again drawing some of this material from the STEMinist_R materials which can be found here 7.1 2.1 Plotting These lessons are evenly divided between live coding and performed by the instructor and exercises performed by the students in class with instructor support. This class will take place with students typing directly into an R script for the exercises all of which can be found in the Week 4 file here You can download just the R files for just this week via wget with the following link wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week6_semester.tar.gz this is a compressed file which can be uncompressed via: tar -xzvf week6_semester.tar.gz A few useful commands that we will cover include: points() lines() abline() hist() boxplot() plot() A few useful arguments within plot(): main, xlab, ylab, col, pch, cex 7.2 Scatterplots Within our msleep dataframe lets plot sleep_total by bodywt (bodyweight) library(ggplot2) data(msleep) plot(msleep$sleep_total,msleep$sleep_rem) # or plot response variable as a function &quot;~&quot; of the predictor variable plot(msleep$sleep_total~msleep$sleep_rem) #you&#39;ll notice this swaps the x and y axis 7.3 Customizing your plot There are several different arguments within plotting functions that can be used to customize your plot. col changes color pch changes point character cex changes size type changes type (l = line, p = points, b = both) lty changes line type bty changes (or removes) the border around the plot (n = no box, 7 = top + right, L = bottom+left, C top+left+bottom, U = left+bottom+right) You can view different point characters with ?pch There are many color options in R. For some general colors you can write the name (blue, red, green, etc). There are apparently 657 named colors in R (including \"slateblue3, and peachpuff4) but you can also use the color hexidecimal code for a given color. There are several comprehensives guides for colors in R online and one of which can be found (here)[https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf] Lets remake the total_sleep against sleep_rem plot and add-in some modifiers #Choose a pch and make the color blue and give it a bottom+left border plot(msleep$sleep_total~msleep$sleep_rem, pch = 16, col=&quot;blue&quot;, bty=&quot;L&quot;) We can change the axes and title labels using xlab, ylab, and main arguments. Lets add labels to our plot. #Choose a pch and make the color blue and give it a bottom+left border plot(msleep$sleep_total~msleep$sleep_rem, pch = 16, col=&quot;blue&quot;, bty=&quot;L&quot;, xlab=&quot;REM Sleep (hours)&quot;, ylab= &quot;Total Sleep (hours)&quot;) You may want to find out which points are on a plot. You can use identify() in place of plot() to identify specific points within your plot. This function prints out the row numbers for the points that you selected. We can also add lines to an existing plot with ablines(). Lets add a line fit from a linear model to our plot. #first make a plot plot(msleep$sleep_total~msleep$sleep_rem, pch = 16, col=&quot;blue&quot;, bty=&quot;L&quot;, xlab=&quot;REM Sleep (hours)&quot;, ylab= &quot;Total Sleep (hours)&quot;) #then add a line. The function lm runs a linear model on our x, y values. abline(lm(msleep$sleep_total~msleep$sleep_rem)) You can add a legend to a plot with legend() which needs you to specify the location. To do this, lets make a cutoff for our points and color them by points above and below the cutoff. Well use our subsetting skills from last week. Feel free to review that section (1.3). #start by defining points by whether they are greater than sleep_total 16 and storing #first make a empty column named colors within the msleep dataframe msleep$colors=NA #store the colors &quot;red&quot; or &quot;black&quot; in the color column for the rows that satsify the following criteria. msleep$colors[msleep$sleep_total &gt;= 17] &lt;-&quot;red&quot; msleep$colors[msleep$sleep_total &lt; 17] &lt;-&quot;black&quot; plot(msleep$sleep_total~msleep$sleep_rem, pch = 16, col=msleep$colors, bty=&quot;L&quot;, xlab=&quot;REM Sleep (hours)&quot;, ylab= &quot;Total Sleep (hours)&quot;) In addition to scatterplots you can make histograms and boxplots in base R. The same parameter options (pch, col, ylab, xlab, etc) apply for these plots as well as scatterplots. R will automatically plot a barplot if you give to the plot() function a continuous variable and a factor. If you have a vector stored as a character converting it to a factor via as.factor will make a boxplot. #let&#39;s make a histogram of sleep_total and fill it with the color palette rainbow() which needs to know how many colors to use hist(msleep$sleep_total, col=rainbow(10)) #let&#39;s make a boxplot of sleep_total and order making eachone a different color (how would you find out how many unique orders are in msleep?) #using plot #plot(msleep$sleep_total~as.factor(msleep$order), col=rainbow(19)) #this is commented out simply to avoid ploting the same plot twice #or boxplot boxplot(msleep$sleep_total~as.factor(msleep$order), col=rainbow(19)) Another example looking at sleep variation across the different types of consumers (carnivore, herbivore, insectivore and omnivore): plot(msleep$sleep_total~as.factor(msleep$vore),col=rainbow(4), xlab=&quot;REM Sleep (hours)&quot;, ylab= &quot;Total Sleep (hours)&quot;) 7.4 Exercise 2.1 Read in the data using data(ChickWeight) # Note: this dataset can also be accessed directly from the ChickWeight package in R # (see ?ChickWeight) data(&quot;ChickWeight&quot;) First, explore the data. How many chicks are in the dataset? How many different diets are in the experiment? Solution length(unique(ChickWeight$Chick)) #&gt; [1] 50 length(unique(ChickWeight$Diet)) #&gt; [1] 4 To vizualize the basics of the data, plot weight versus time Solution plot(ChickWeight$weight ~ ChickWeight$Time, xlab = &quot;Time (days)&quot;, ylab = &quot;Weight (gm)&quot;) Plot a histogram of the weights of the chicks at the final day of the experiments (i.e. only the chicks who made it to the last day) Solution par(mfrow = c(1,1)) hist(ChickWeight$weight[ChickWeight$Time == max(ChickWeight$Time)], xlab = &quot;Weight (gm)&quot;, main = &quot;Weights at final day of experiment&quot;, col = rainbow(10)) Create a boxplot where the x-axis represents the different diets and the y-axis is the weights of the chicks at the final day of the experiments Solution my.new = ChickWeight[ChickWeight$Time == max(ChickWeight$Time), ] boxplot(weight ~ Diet, data = my.new, xlab = &quot;Diet&quot;, ylab = &quot;Weight (gm)&quot;, main = &quot;Final weights given diet type&quot;, col = c(&quot;red&quot;, &quot;blue&quot;, &quot;orange&quot;, &quot;green&quot;)) Try using the package R Color Brewer to generate color palettes. Go to http://colorbrewer2.org/ to vizualize palettes. You can choose palettes that are colorblind safe, print friendly, etc. # Install R Color Brewer #install.packages(&quot;RColorBrewer&quot;) library(&quot;RColorBrewer&quot;) Define a color pallete with 10 colors and re-plot the histogram of the weights of the chicks at the final day of the experiments in these colors Note: if histogram has n breaks and n is less than 10, it will just use first n colors. If n is greater than 10, it will reuse colors. Solution library(RColorBrewer) my.colors = brewer.pal(10, &quot;Paired&quot;) hist(ChickWeight$weight[ChickWeight$Time == max(ChickWeight$Time)], xlab = &quot;Weight (gm)&quot;,main = &quot;Weights at final day of experiment&quot;, col = my.colors) 7.5 2.2 plotting with ggplot2 GGPlot is a package that allows you to make a lot of different kinds of plots and has become increasingly popular. There are also many tutorials on how to use ggplot as well as example code that could be modified to fit the data youre interested in plotting. There is a really helpful cheatsheat (here)[https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf] There is a little bit of a learning curve for ggplot as the syntax is structured differently than base R plotting. One thing that remains the same and is even more noticible in ggplot is the iterative process of building a plot, one aspect at a time. Lets demonstrate what ggplot can do with the states data set #load in the data data(state) states = as.data.frame(state.x77) # convert data to a familiar format - data frame str(states) # let&#39;s take a look at the dataframe #&gt; &#39;data.frame&#39;: 50 obs. of 8 variables: #&gt; $ Population: num 3615 365 2212 2110 21198 ... #&gt; $ Income : num 3624 6315 4530 3378 5114 ... #&gt; $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... #&gt; $ Life Exp : num 69 69.3 70.5 70.7 71.7 ... #&gt; $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... #&gt; $ HS Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... #&gt; $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... #&gt; $ Area : num 50708 566432 113417 51945 156361 ... #make an initial ggplot ggplot(data=states) We just see a grey box. In order to tell ggplot what to put in the box we use the aes(). The aes() function stands for aesthetics and will be used to specify our axes and how we want the data grouped. #lets make a scatterplot of population and income #we specify which axes we want to be x and y with aes() #we&#39;ll then use geom_point to tell it to make a scatterplot using the data we specified in the first command ggplot(data=states, aes(x=Population, y=Income))+geom_point() There are many types of plots in ggplot that can be called with geom_ including geom_line, geom_boxplot geom_bar and many others! Lets add a line to our plot that of best fit for Population ~ Income. Each time we add something to our plot we use the + sign. Well use geom_smooth() to draw a line with the method for lm which stands for linear model. ggplot(data=states, aes(x=Population, y=Income))+geom_point()+geom_smooth(method=&quot;lm&quot;) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; As you can already see ggplot works with many more parameters drawn in default than plotting in base R. For example, the background of our plot is grey the confidence interval of our line is drawn for us and is shaded dark grey and the line of best fit is in blue. All of these things can be modified if we wish. Many of these options can easily be changed with the theme_ functions. Lets change to a minimal theme which removes the gray background in the back of the plot. Play around with the other themes to see what they change. ggplot(data=states, aes(x=Population, y=Income))+geom_point()+geom_smooth(method=&quot;lm&quot;)+theme_minimal() #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; Another plot example: ggplot(data=states, aes(x=Income, y=Illiteracy, color=Population)) +geom_point()+geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;)+theme_classic() #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; Lets use the msleep data set to explore what ggplot can do with character vectors. Make a plot of total sleep against REM sleep and then group by vore. # because our vore vector is a character vector we must convert it to a factor before we can use it to group or color ggplot(msleep, aes(y=sleep_total, x=sleep_rem, group=as.factor(vore), color=as.factor(vore))) +geom_point() #&gt; Warning: Removed 22 rows containing missing values (geom_point). That looks fine, but we may want to add axis labels and change the legend. The code below does just that and changes the theme. # as we add things to the plot the line can get really long, you can hit enter after the plus sign to start a new line ggplot(msleep, aes(y=sleep_total, x=sleep_rem, group=as.factor(vore), color=as.factor(vore)))+ geom_point()+ labs(y= &quot;Total Sleep (hours)&quot;, x= &quot;REM sleep (hours)&quot;)+ theme_minimal()+ scale_color_manual(name=&quot;Diet&quot;, labels = c(&quot;Carnivore&quot;, &quot;Herbivore&quot;, &quot;Insectivore&quot;, &quot;Omnivore&quot;, &quot;NA&quot;), values = c(&quot;carni&quot;=&quot;blue&quot;, &quot;herbi&quot;=&quot;red&quot;, &quot;insecti&quot;=&quot;green&quot;, &quot;omni&quot;=&quot;brown&quot;, &quot;NA&quot;=&quot;orange&quot;)) #&gt; Warning: Removed 22 rows containing missing values (geom_point). Our plot at this point is getting very clunky. You can assign what we have so far to an object and continue to add parameters without having to copy and paste the whole plot. #assign to an object g&lt;-ggplot(msleep, aes(y=sleep_total, x=sleep_rem, group=as.factor(vore), color=as.factor(vore)))+ geom_point()+ labs(y= &quot;Total Sleep (hours)&quot;, x= &quot;REM sleep (hours)&quot;)+ theme_minimal()+ scale_color_manual(name=&quot;Diet&quot;, labels = c(&quot;Carnivore&quot;, &quot;Herbivore&quot;, &quot;Insectivore&quot;, &quot;Omnivore&quot;, &quot;NA&quot;), values = c(&quot;carni&quot;=&quot;blue&quot;, &quot;herbi&quot;=&quot;red&quot;, &quot;insecti&quot;=&quot;green&quot;, &quot;omni&quot;=&quot;brown&quot;, &quot;NA&quot;=&quot;orange&quot;)) g #&gt; Warning: Removed 22 rows containing missing values (geom_point). One final example to share. I use ggplot often with data sets that have multiple character vectors and I want to see how they relate to my continuous variables. For example in the iris dataframe we may be interested in looking at the relationship between Sepal.Length and Sepal.Width for each species. You can look at all of these together with facet_wrap or facet_grid. ggplot(iris, aes(y=Sepal.Length, x=Sepal.Width, group=Species, color=Species))+ geom_point()+ facet_wrap(~Species)+ geom_smooth(method=&quot;lm&quot;) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; Finally in ggplot we may be interested in seeing the mean values plotted with error bars for several groups. You can use the function stat_summary to find the mean and error around that mean for the given grouping. Heres a plot looking at the mean chickweight by diet. ggplot(ChickWeight, aes(x=Time, y=weight, group=Diet, color=Diet))+ stat_summary(fun=mean, geom=&quot;point&quot;, size=1)+ stat_summary(fun=mean, geom=&quot;line&quot;, size=1)+ stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, aes(width=0.1), size=0.5) 7.6 Exercise 2.2 Plotting in ggplot2 Add best fit lines to our msleep plot for each vore. Solution # we can just use the geom_smooth command from above and ggplot takes care of the rest! # The code below will only work if you stored your plot in object g. g+geom_smooth(method=&quot;lm&quot;) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 22 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 22 rows containing missing values (geom_point). In the msleep data, make a boxplot of sleep_total against vore. Make sure vore is a factor. Color the boxplots by vore (remember how we had to color the boxplots in base R) it is similar in ggplot. Solution ggplot(msleep, aes(y=sleep_total, x=as.factor(vore), fill=as.factor(vore)))+geom_boxplot() Load a new dataframe midwest(run data(midwest)) and plot a scatterplot of area against popdensity grouped and color by state. Do a facet grid by state. Solution ggplot(midwest, aes(y=area, x=popdensity, col=as.factor(state)))+geom_point()+facet_grid(~state) In the midwest dataframe make a scatterplot of the popdensity by poptotal of only IL (this requires you to subset your data for illinois). Solution #we can subset our data first and store the subset in a new dataframe il&lt;-midwest[midwest$state == &quot;IL&quot;,] ggplot(il, aes(x=popdensity, y=poptotal))+geom_point() In the midwest dataframe plot the mean and standard error for popdensity for each state. Color this plot in your favorite palette. Solution ggplot(midwest, aes(x=state, y=popdensity, color=state))+ stat_summary(fun.y=mean, geom=&quot;point&quot;, size=1)+ stat_summary(fun.y=mean, geom=&quot;line&quot;, size=1)+ stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, aes(width=0.1), size=0.5)+ theme_minimal()+ scale_color_manual(values=rainbow(5)) #&gt; Warning: `fun.y` is deprecated. Use `fun` instead. #&gt; Warning: `fun.y` is deprecated. Use `fun` instead. #&gt; geom_path: Each group consists of only one observation. Do you need to adjust the group aesthetic? "],["week-7-for-loops-and-the-apply-family-of-functions.html", "8 Week 7 For loops and the apply family of functions 8.1 Exercise 2.3 apply and tapply Exercise 2.3", " 8 Week 7 For loops and the apply family of functions A few useful commands: function(), is.na, which, var, length, for(){ }, points, print, paste, plot, unique, sample for loops: In many languages, the best way to repeat a calculation is to use a for-loop: For example, we could square each number 1 to 10 squares = rep(NA, 10) # use rep to create a vector length 10 of NAs to store the result for (i in 1:10) { # for loop squares[i] = i^2 } squares #&gt; [1] 1 4 9 16 25 36 49 64 81 100 An alternative to for-loops in R is using the apply family, while for-loops apply a function to one item at a time and then go on to the next one, apply applies functions to every item at once ## apply family ### sapply There are several apply functions that vary in the output the return and vary somewhat in the input they require. Well go over sapply simplifying apply which returns a vector, first. #?sapply # syntax: sapply(X = object_to_repeat_over, FUN = function_to_repeat) # simple example of sapply over a vector # we can use an in-line function definition sapply(1:10, function(x) x^2) #&gt; [1] 1 4 9 16 25 36 49 64 81 100 # equivalently, we can define our own functions separately for sapply # e.g. a function that calculates the area of a circle radius r, pi*r^2 areaCircle = function(r){ return(pi * r^2) } sapply(1:10, areaCircle) #&gt; [1] 3.141593 12.566371 28.274334 50.265482 78.539816 113.097336 153.938040 201.061930 254.469005 314.159265 # in R, we can also just use short-hand for simple vector calculations: pi*(1:10)^2 #&gt; [1] 3.141593 12.566371 28.274334 50.265482 78.539816 113.097336 153.938040 201.061930 254.469005 314.159265 # but unlike the short-hand, sapply can also iterate over elements in a list listy = list(a = TRUE, b = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), c = 10:100) str(listy) # look at the structure of &#39;listy&#39; #&gt; List of 3 #&gt; $ a: logi TRUE #&gt; $ b: chr [1:3] &quot;a&quot; &quot;b&quot; &quot;c&quot; #&gt; $ c: int [1:91] 10 11 12 13 14 15 16 17 18 19 ... length(listy) # look at the length of &#39;listy&#39; #&gt; [1] 3 # use sapply to return a vector for length of each object within the list sapply(listy, FUN = length) #&gt; a b c #&gt; 1 3 91 You can also use sapply to create plots! For example, use sapply to plot these 4 dataframes at once: df1 = data.frame(x1 = 1:10, y1 = 1:10) df2 = data.frame(x2 = 1:10, y2 = -1:-10) df3 = data.frame(x3 = 1:10, y3 = 10:1) df4 = data.frame(x4 = 1:10, y4 = 1:10) my_list = list(df1, df2, df3, df4) # put 4 data frames together in a list par(mfrow = c(2,2)) # set up frame for 4 plots sapply(my_list, plot) # plot my_list with sapply #&gt; [[1]] #&gt; NULL #&gt; #&gt; [[2]] #&gt; NULL #&gt; #&gt; [[3]] #&gt; NULL #&gt; #&gt; [[4]] #&gt; NULL 8.0.1 apply The apply function is highly useful for applying a function to rows or columns of a dataframe or matrix. Example syntax for the dataframe or matrix X: apply(X = over this object, MARGIN 1 for rows or 2 for columns,FUN = apply this function) You can also use apply on a dataframe we worked with earlier the states data to plot each column against Population #load in the data data(state) states = as.data.frame(state.x77) # convert data to a familiar format - data frame str(states) # let&#39;s take a look at the dataframe #&gt; &#39;data.frame&#39;: 50 obs. of 8 variables: #&gt; $ Population: num 3615 365 2212 2110 21198 ... #&gt; $ Income : num 3624 6315 4530 3378 5114 ... #&gt; $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... #&gt; $ Life Exp : num 69 69.3 70.5 70.7 71.7 ... #&gt; $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... #&gt; $ HS Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... #&gt; $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... #&gt; $ Area : num 50708 566432 113417 51945 156361 ... # calculate the mean for each column apply(states, 2, mean) #&gt; Population Income Illiteracy Life Exp Murder HS Grad Frost Area #&gt; 4246.4200 4435.8000 1.1700 70.8786 7.3780 53.1080 104.4600 70735.8800 # note you could get this with colMeans() or summary(), along with the min and max and other values, but there may be instances where you only want the mean # you could also plot each column against Population in ggplot apply(states, 2, FUN = function(i) ggplot(states, aes(x=Population, y = i))+geom_point()+geom_smooth(method=&quot;lm&quot;)+theme_classic()) #&gt; $Population #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $Income #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $Illiteracy #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $`Life Exp` #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $Murder #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $`HS Grad` #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $Frost #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; #&gt; $Area #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; We can do the same things across all rows. But if you want to plot all the rows as we did the columns above, I suggest you do that with a smaller dataset than the states dataframe. #calculate the sum across each row in states apply(states, 1, sum) #&gt; Alabama Alaska Arizona Arkansas California Colorado Connecticut Delaware #&gt; 58094.55 573412.81 120312.25 57620.56 182838.71 111500.46 13581.68 7604.76 #&gt; Florida Georgia Hawaii Idaho Illinois Indiana Iowa Kansas #&gt; 67328.26 67280.04 12399.60 87872.27 72312.94 46121.58 63704.36 88987.58 #&gt; Kentucky Louisiana Maine Maryland Massachusetts Michigan Minnesota Mississippi #&gt; 46964.80 52419.96 35961.49 19544.92 18632.73 70939.43 88178.46 52908.99 #&gt; Missouri Montana Nebraska Nevada New Hampshire New Jersey New Mexico New York #&gt; 78253.59 150970.36 82809.40 115962.23 14426.83 20335.73 126414.42 71027.55 #&gt; North Carolina North Dakota Ohio Oklahoma Oregon Pennsylvania Rhode Island South Carolina #&gt; 58314.61 75308.28 56527.22 75692.52 103308.93 61528.73 6787.00 36860.66 #&gt; South Dakota Tennessee Texas Utah Vermont Virginia Washington West Virginia #&gt; 81102.58 49516.61 278726.70 87603.30 13948.84 49675.78 75165.12 29705.18 #&gt; Wisconsin Wyoming #&gt; 63800.68 102458.69 8.0.2 lapply  list apply Well just show a quick example of lapply. It works in the same way as sapply, but returns a list instead of a vector. lapply(1:10, function(x) x^2) # lapply returns list #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 9 #&gt; #&gt; [[4]] #&gt; [1] 16 #&gt; #&gt; [[5]] #&gt; [1] 25 #&gt; #&gt; [[6]] #&gt; [1] 36 #&gt; #&gt; [[7]] #&gt; [1] 49 #&gt; #&gt; [[8]] #&gt; [1] 64 #&gt; #&gt; [[9]] #&gt; [1] 81 #&gt; #&gt; [[10]] #&gt; [1] 100 sapply(1:10, function(x) x^2, simplify = FALSE) # same as an lapply #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 9 #&gt; #&gt; [[4]] #&gt; [1] 16 #&gt; #&gt; [[5]] #&gt; [1] 25 #&gt; #&gt; [[6]] #&gt; [1] 36 #&gt; #&gt; [[7]] #&gt; [1] 49 #&gt; #&gt; [[8]] #&gt; [1] 64 #&gt; #&gt; [[9]] #&gt; [1] 81 #&gt; #&gt; [[10]] #&gt; [1] 100 sapply(1:10, function(x) x^2) # default is simplify = TRUE which retuns a vector #&gt; [1] 1 4 9 16 25 36 49 64 81 100 8.0.3 tapply - per Type apply The tapply function is one of my favorites because it is a really great way to sumarize data that has multiple categorical variables that can be # load state data again, you can skip this if you already have it loaded data(state) states = as.data.frame(state.x77) # convert data to a familiar format - data frame str(states) # let&#39;s take a look at the dataframe #&gt; &#39;data.frame&#39;: 50 obs. of 8 variables: #&gt; $ Population: num 3615 365 2212 2110 21198 ... #&gt; $ Income : num 3624 6315 4530 3378 5114 ... #&gt; $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... #&gt; $ Life Exp : num 69 69.3 70.5 70.7 71.7 ... #&gt; $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... #&gt; $ HS Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... #&gt; $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... #&gt; $ Area : num 50708 566432 113417 51945 156361 ... # example syntax --- tapply(variable of interest, grouping variable, function) # for each US region in our dataset, finds the mean of Frost for states in that region tapply(states$Frost, state.region, mean) # state.region contains the region information for each state #&gt; Northeast South North Central West #&gt; 132.7778 64.6250 138.8333 102.1538 # you can nest apply statements! Let&#39;s find the region average for all the variables in the states dataset apply(states, 2, # apply over columns of my_states function(x) tapply(x, state.region, mean)) # each column = variable of interest for tapply #&gt; Population Income Illiteracy Life Exp Murder HS Grad Frost Area #&gt; Northeast 5495.111 4570.222 1.000000 71.26444 4.722222 53.96667 132.7778 18141.00 #&gt; South 4208.125 4011.938 1.737500 69.70625 10.581250 44.34375 64.6250 54605.12 #&gt; North Central 4803.000 4611.083 0.700000 71.76667 5.275000 54.51667 138.8333 62652.00 #&gt; West 2915.308 4702.615 1.023077 71.23462 7.215385 62.00000 102.1538 134463.00 8.1 Exercise 2.3 apply and tapply Exercise 2.3 A few useful commands: function(){ }, apply(), tapply(), hist(), dim(), prod(), sd() 1. what is the average population, income, and area of all 50 states ins the states dataset Solution # load state data #?state data(state) # this data is stored in a slightly different way than other datasets we&#39;ve used so far states = as.data.frame(state.x77) # run this line of code to avoid later confusion apply(states,2,mean) #&gt; Population Income Illiteracy Life Exp Murder HS Grad Frost Area #&gt; 4246.4200 4435.8000 1.1700 70.8786 7.3780 53.1080 104.4600 70735.8800 #or an alternative that will get you only the columns requested colMeans(states[,c(&quot;Population&quot;, &quot;Income&quot;, &quot;Area&quot;)]) #&gt; Population Income Area #&gt; 4246.42 4435.80 70735.88 what is the average area of the states from different regions of the country? Hint: use the object state.region in your environment Solution tapply(states$Area, state.region, mean) #&gt; Northeast South North Central West #&gt; 18141.00 54605.12 62652.00 134463.00 Plot a histogram for each column in the states data (Population, Income, Illiteracy etc.) Solution #how many columns do we have? dim(states) #&gt; [1] 50 8 par(mfrow = c(2,2)) # make your plot window show 2 rows and 2 columns at once apply(states, 2, hist) #&gt; $Population #&gt; $breaks #&gt; [1] 0 5000 10000 15000 20000 25000 #&gt; #&gt; $counts #&gt; [1] 38 6 4 1 1 #&gt; #&gt; $density #&gt; [1] 0.000152 0.000024 0.000016 0.000004 0.000004 #&gt; #&gt; $mids #&gt; [1] 2500 7500 12500 17500 22500 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $Income #&gt; $breaks #&gt; [1] 3000 3500 4000 4500 5000 5500 6000 6500 #&gt; #&gt; $counts #&gt; [1] 2 11 11 18 7 0 1 #&gt; #&gt; $density #&gt; [1] 0.00008 0.00044 0.00044 0.00072 0.00028 0.00000 0.00004 #&gt; #&gt; $mids #&gt; [1] 3250 3750 4250 4750 5250 5750 6250 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $Illiteracy #&gt; $breaks #&gt; [1] 0.5 1.0 1.5 2.0 2.5 3.0 #&gt; #&gt; $counts #&gt; [1] 26 11 7 5 1 #&gt; #&gt; $density #&gt; [1] 1.04 0.44 0.28 0.20 0.04 #&gt; #&gt; $mids #&gt; [1] 0.75 1.25 1.75 2.25 2.75 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $`Life Exp` #&gt; $breaks #&gt; [1] 67 68 69 70 71 72 73 74 #&gt; #&gt; $counts #&gt; [1] 1 3 5 21 8 11 1 #&gt; #&gt; $density #&gt; [1] 0.02 0.06 0.10 0.42 0.16 0.22 0.02 #&gt; #&gt; $mids #&gt; [1] 67.5 68.5 69.5 70.5 71.5 72.5 73.5 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $Murder #&gt; $breaks #&gt; [1] 0 2 4 6 8 10 12 14 16 #&gt; #&gt; $counts #&gt; [1] 2 9 8 10 4 12 4 1 #&gt; #&gt; $density #&gt; [1] 0.02 0.09 0.08 0.10 0.04 0.12 0.04 0.01 #&gt; #&gt; $mids #&gt; [1] 1 3 5 7 9 11 13 15 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $`HS Grad` #&gt; $breaks #&gt; [1] 35 40 45 50 55 60 65 70 #&gt; #&gt; $counts #&gt; [1] 4 6 4 15 13 5 3 #&gt; #&gt; $density #&gt; [1] 0.016 0.024 0.016 0.060 0.052 0.020 0.012 #&gt; #&gt; $mids #&gt; [1] 37.5 42.5 47.5 52.5 57.5 62.5 67.5 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $Frost #&gt; $breaks #&gt; [1] 0 20 40 60 80 100 120 140 160 180 200 #&gt; #&gt; $counts #&gt; [1] 6 2 3 4 5 7 11 4 6 2 #&gt; #&gt; $density #&gt; [1] 0.006 0.002 0.003 0.004 0.005 0.007 0.011 0.004 0.006 0.002 #&gt; #&gt; $mids #&gt; [1] 10 30 50 70 90 110 130 150 170 190 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; #&gt; #&gt; $Area #&gt; $breaks #&gt; [1] 0e+00 1e+05 2e+05 3e+05 4e+05 5e+05 6e+05 #&gt; #&gt; $counts #&gt; [1] 42 6 1 0 0 1 #&gt; #&gt; $density #&gt; [1] 8.4e-06 1.2e-06 2.0e-07 0.0e+00 0.0e+00 2.0e-07 #&gt; #&gt; $mids #&gt; [1] 50000 150000 250000 350000 450000 550000 #&gt; #&gt; $xname #&gt; [1] &quot;newX[, i]&quot; #&gt; #&gt; $equidist #&gt; [1] TRUE #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;histogram&quot; lets assume that we dont want to live in a state with high illiteracy, high murder, and many freezing days; also assume that each of these factors contribute equally to our opinion (Illiteracy * Murder * Frost) = undesirable What 10 states should we avoid? # hint use prod(); and maybe order() Solution livability &lt;- apply(states[,c(&quot;Illiteracy&quot;, &quot;Murder&quot;, &quot;Frost&quot;)], 1, prod) # subset to variables of interest livability[order(livability, decreasing = T)][1:10] # top ten least livable states #&gt; Alaska New Mexico South Carolina Georgia Kentucky North Carolina Mississippi Tennessee #&gt; 2576.40 2560.80 1734.20 1668.00 1611.20 1598.40 1500.00 1309.00 #&gt; New York Michigan #&gt; 1251.32 1248.75 use sapply() to plot a histogram of the data below 4 times, in 4 different colors. For extra style, title the plot by its color, e.g. the red plot is titled red data_to_plot = c(1,3,4,5,6,3,3,4,5,1,1,1,1,1) par(mfrow = c(2,2))# run this line to set your plot to make 4 plots in total (2rows, 2columns) Solution data_to_plot = c(1,3,4,5,6,3,3,4,5,1,1,1,1,1) my_colors = c(&quot;deeppink&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;) par(mfrow = c(2,2)) # extra styling, plots in a 2x2 grid sapply(my_colors, FUN = function(i) hist(data_to_plot, main = i, col = i)) #&gt; deeppink red blue darkgreen #&gt; breaks Integer,6 Integer,6 Integer,6 Integer,6 #&gt; counts Integer,5 Integer,5 Integer,5 Integer,5 #&gt; density Numeric,5 Numeric,5 Numeric,5 Numeric,5 #&gt; mids Numeric,5 Numeric,5 Numeric,5 Numeric,5 #&gt; xname &quot;data_to_plot&quot; &quot;data_to_plot&quot; &quot;data_to_plot&quot; &quot;data_to_plot&quot; #&gt; equidist TRUE TRUE TRUE TRUE Standardize all the variables in the states dataset and save your answer to a new dataframe, states_standardized Hint: to standardize a variable, you subtract the mean and divide by the standard deviation (sd) Solution states_standardized = apply(states, 2, function(x) (x-mean(x))/sd(x)) # original: head(states) #&gt; Population Income Illiteracy Life Exp Murder HS Grad Frost Area #&gt; Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 #&gt; Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 #&gt; Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 #&gt; Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 #&gt; California 21198 5114 1.1 71.71 10.3 62.6 20 156361 #&gt; Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 # standardized head(states_standardized) #&gt; Population Income Illiteracy Life Exp Murder HS Grad Frost Area #&gt; Alabama -0.1414316 -1.3211387 1.525758 -1.3621937 2.0918101 -1.4619293 -1.6248292 -0.2347183 #&gt; Alaska -0.8693980 3.0582456 0.541398 -1.1685098 1.0624293 1.6828035 0.9145676 5.8093497 #&gt; Arizona -0.4556891 0.1533029 1.033578 -0.2447866 0.1143154 0.6180514 -1.7210185 0.5002047 #&gt; Arkansas -0.4785360 -1.7214837 1.197638 -0.1628435 0.7373617 -1.6352611 -0.7591257 -0.2202212 #&gt; California 3.7969790 1.1037155 -0.114842 0.6193415 0.7915396 1.1751891 -1.6248292 1.0034903 #&gt; Colorado -0.3819965 0.7294092 -0.771082 0.8800698 -0.1565742 1.3361400 1.1838976 0.3870991 Create a histogram again for each variable in the states data, but this time label each histogram with the variable names when you plot Hint: instead of using apply to iterate over the columns themselves, you can often iterate over the column names with sapply Solution par(mfrow = c(2,2)) sapply(colnames(states), function(x)hist(states[ , x],main = x, xlab = x,col = &quot;darkblue&quot;)) #&gt; Population Income Illiteracy Life Exp Murder HS Grad Frost Area #&gt; breaks Numeric,6 Integer,8 Numeric,6 Integer,8 Numeric,9 Integer,8 Numeric,11 Numeric,7 #&gt; counts Integer,5 Integer,7 Integer,5 Integer,7 Integer,8 Integer,7 Integer,10 Integer,6 #&gt; density Numeric,5 Numeric,7 Numeric,5 Numeric,7 Numeric,8 Numeric,7 Numeric,10 Numeric,6 #&gt; mids Numeric,5 Numeric,7 Numeric,5 Numeric,7 Numeric,8 Numeric,7 Numeric,10 Numeric,6 #&gt; xname &quot;states[, x]&quot; &quot;states[, x]&quot; &quot;states[, x]&quot; &quot;states[, x]&quot; &quot;states[, x]&quot; &quot;states[, x]&quot; &quot;states[, x]&quot; &quot;states[, x]&quot; #&gt; equidist TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE For the final day covering basics in R we will have a coding session where we tackle a challenge exercise on a dataset. Please download the exercise here wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/R_Final_Challenges/chickWeightChallenge_week7_semester.R And the solutions are here wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/R_Final_Challenges/ChickWeight_Challenge_SOLUTIONS.pdf "],["week-8--principle-component-analyses.html", "9 Week 8- Principle Component Analyses 9.1 Download the data 9.2 Installing programs 9.3 Run pcangsd on our data 9.4 Setting up a new project in R 9.5 Reading data into R 9.6 Exercises", " 9 Week 8- Principle Component Analyses The lecture for this week focused on Plotting a PCA can be found here. In week 5 we mapped reads to a genome so we could call our genetic variants (SNPs) and generated bam files for each read file (= sequence data from an individual). Now that we know how to do some R basics we can use those SNPs to plot patterns of population structure via a principal component analysis. The first portion of this lesson will be performed in bash and then we will generate the plots in RStudio. 9.1 Download the data We first need to download the data. Use the link below to download it to jetstream, and then us the tar command to un-compress it. wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week8_semester.tar.gz tar -xzvf week9_semester.tar.gz Our data this week consists of genotype_likelihood files in beagle format and a bam.filelist which contains id information for our samples. ls -lh MarineGenomicsData/Week8 9.2 Installing programs We will also need to install a few programs. The code below installs the program pcangsd, and a few dependencies for pcangsd: #install pip for python curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py #we need to add our home directory to the path for pip #look at path echo $PATH #add the location of pip to our path export PATH=&quot;$HOME/.local/bin:$PATH&quot; #then install pcangsd git clone https://github.com/Rosemeis/pcangsd.git cd pcangsd/ pip install --user -r requirements.txt #this installs additional requirements for pcangsd python3 setup.py build_ext --inplace Check that its installed correctly. #navigate to your home directory cd python3 pcangsd/pcangsd.py -h You should see the help menu for pcangsd. 9.3 Run pcangsd on our data python3 ../../pcangsd/pcangsd.py -beagle genolike_beagle.beagle.gz -o pca_out -threads 28 This will generate a covariance matrix output file. We can read this file into R and compute the eigenvectors and eigenvalues and make our plot. 9.4 Setting up a new project in R Well first open a new .R script to keep track of our code and everything that were doing. Be sure to comment heavily with #. It may seem obvious what were doing now, but you will forget what certain lines of code do in a surprisingly short amount of time. 9.5 Reading data into R There are several ways to read data in R. Useful functions include read.table(), getwd(), and setwd() R like many other programs is set to start in a specific directory, usually the desktop or home directory. You can check and see what directory R is set in with the command getwd() And of course all of this code is typed into our script and commented along the way. #check working directory #getwd() #should output #[1] &quot;/home/margeno&quot; Thus in jetstream the working directory is set to our home directory, which will work fine for uploading data. If you need to change the directory you can use setwd() and provide it with the full directory path (e.g., \"C:\") and then confirm that it moved the directory with getwd(). In Rstudio you can also use the pull down menus Session to get and set your working directory. Similarly, the tab Files in the lower right will also allow you to set your working directory. Now well read the data output by angsd into R using read.table() and as.matrix() cov&lt;-as.matrix(read.table(&quot;pca_out.cov&quot;)) And then we can compute the eigenvalues from our covariance matrix with the function eigen. e&lt;-eigen(cov) And make a simple plot in base R plot(e$vectors[,1:2]) We may be interested in how much of the variance our first two components explain. You can look at this in the e object under values. We divide the eigen values by the sum of all the values to get the percent explained by each value. e$values/sum(e$values) #&gt; [1] 0.24278761 0.07662213 0.07502757 0.07349541 0.07287943 0.07034540 0.06674509 0.06530546 0.05280345 0.04958515 0.04274839 #&gt; [12] 0.03706328 0.03527164 0.03241500 0.00690499 We now want to make the colors match population labels. The information for which population each individual sample came from is in the bam.filelist file. #read in the data names&lt;-read.table(&quot;bam.filelist&quot;) #assign the rownames of the covariance matix the rownames(cov)&lt;-names$V2 #remake the plot with the colors we want plot(e$vectors[,1:2], col=as.factor(rownames(cov)), pch=16) There is another column in the bam.filelist file that will allow us to color the populations based on region. Go ahead and modify your plots to have nice x and y labels that state the percent variance explained by each axis, and that are colored by region. You can also change the pch and any other parameters you would like. 9.6 Exercises In R, upload the genotype likelihood maf file (genolike.beagle.maf.gz). Filter the data so that we only use sites for which all 15 individuals have data present (i.e., subsite by the nInd column). How many SNPs does this leave us? How many SNPs did we start with? Hint use the command read.table(gzfile(\"path/to/genotypelikelihoodfile/), header=T) to read in the file in R. Dont forget to assign it to an object or R will print the whole thing to the screen. Solution # read in the data #read in genotype likelihood data and filter by all individuals gen&lt;-read.table(gzfile(&quot;genolike_beagle.mafs.gz&quot;), header=T) #filter by nind == 15 gen_allIND&lt;-gen[gen$nInd==15,] #find how many SNPs we had in the first dataset dim(gen) #&gt; [1] 282950 7 #[1] 282950 7 #how many do we have in the filtered dataset? dim(gen_allIND) #&gt; [1] 31905 7 #[1] 31905 7 Use the filtered file from 01 in pcangsd to recalculate the covarianve matrix and regenerate the pca plot. How does it differ from the one we generated in class? Hint this will require you to merge the filtered data frame from exercise 1 with the beagle formated dataframe. Use the function merge(x, y, by = ), second hint, the function merge only works (properly) if the column names are the same. Change the column names to match across the two dataframes where they have the same info. Solution ## read in the beagle formatted data beag&lt;-read.table(gzfile(&quot;genolike_beagle.beagle.gz&quot;), header=T) #look at the column names between the two datasets colnames(gen) #&gt; [1] &quot;chromo&quot; &quot;position&quot; &quot;major&quot; &quot;minor&quot; &quot;knownEM&quot; &quot;pK.EM&quot; &quot;nInd&quot; colnames(beag) #&gt; [1] &quot;marker&quot; &quot;allele1&quot; &quot;allele2&quot; &quot;Ind0&quot; &quot;Ind0.1&quot; &quot;Ind0.2&quot; &quot;Ind1&quot; &quot;Ind1.1&quot; &quot;Ind1.2&quot; &quot;Ind2&quot; &quot;Ind2.1&quot; &quot;Ind2.2&quot; #&gt; [13] &quot;Ind3&quot; &quot;Ind3.1&quot; &quot;Ind3.2&quot; &quot;Ind4&quot; &quot;Ind4.1&quot; &quot;Ind4.2&quot; &quot;Ind5&quot; &quot;Ind5.1&quot; &quot;Ind5.2&quot; &quot;Ind6&quot; &quot;Ind6.1&quot; &quot;Ind6.2&quot; #&gt; [25] &quot;Ind7&quot; &quot;Ind7.1&quot; &quot;Ind7.2&quot; &quot;Ind8&quot; &quot;Ind8.1&quot; &quot;Ind8.2&quot; &quot;Ind9&quot; &quot;Ind9.1&quot; &quot;Ind9.2&quot; &quot;Ind10&quot; &quot;Ind10.1&quot; &quot;Ind10.2&quot; #&gt; [37] &quot;Ind11&quot; &quot;Ind11.1&quot; &quot;Ind11.2&quot; &quot;Ind12&quot; &quot;Ind12.1&quot; &quot;Ind12.2&quot; &quot;Ind13&quot; &quot;Ind13.1&quot; &quot;Ind13.2&quot; &quot;Ind14&quot; &quot;Ind14.1&quot; &quot;Ind14.2&quot; annoyingly the beagle formatted data merges the chromo and position columns of the maf dataframe two solutions: 1. we could split the columns in beag to match chromo and position 2. we merge the columns in gen to match the format in beag since we need the beagle format to run pcangsd lets do option 2. well use the paste() function to to this well make a new column in gen that is labeled marker to match the beagle column gen_allIND$marker&lt;-paste(gen_allIND$chromo, gen_allIND$position, sep=&quot;_&quot;) #then merge them by the marker column beag_allIND&lt;-merge(gen_allIND, beag, by=&quot;marker&quot;) #that should return a dataframe with 31905 rows and 55 columns #pcangsd doesn&#39;t want those extra columns that came from our merge #lets get rid of them to leave the 48 columns that beagle needs. #use colnames to see which ones we want colnames(beag_allIND) #&gt; [1] &quot;marker&quot; &quot;chromo&quot; &quot;position&quot; &quot;major&quot; &quot;minor&quot; &quot;knownEM&quot; &quot;pK.EM&quot; &quot;nInd&quot; &quot;allele1&quot; &quot;allele2&quot; &quot;Ind0&quot; #&gt; [12] &quot;Ind0.1&quot; &quot;Ind0.2&quot; &quot;Ind1&quot; &quot;Ind1.1&quot; &quot;Ind1.2&quot; &quot;Ind2&quot; &quot;Ind2.1&quot; &quot;Ind2.2&quot; &quot;Ind3&quot; &quot;Ind3.1&quot; &quot;Ind3.2&quot; #&gt; [23] &quot;Ind4&quot; &quot;Ind4.1&quot; &quot;Ind4.2&quot; &quot;Ind5&quot; &quot;Ind5.1&quot; &quot;Ind5.2&quot; &quot;Ind6&quot; &quot;Ind6.1&quot; &quot;Ind6.2&quot; &quot;Ind7&quot; &quot;Ind7.1&quot; #&gt; [34] &quot;Ind7.2&quot; &quot;Ind8&quot; &quot;Ind8.1&quot; &quot;Ind8.2&quot; &quot;Ind9&quot; &quot;Ind9.1&quot; &quot;Ind9.2&quot; &quot;Ind10&quot; &quot;Ind10.1&quot; &quot;Ind10.2&quot; &quot;Ind11&quot; #&gt; [45] &quot;Ind11.1&quot; &quot;Ind11.2&quot; &quot;Ind12&quot; &quot;Ind12.1&quot; &quot;Ind12.2&quot; &quot;Ind13&quot; &quot;Ind13.1&quot; &quot;Ind13.2&quot; &quot;Ind14&quot; &quot;Ind14.1&quot; &quot;Ind14.2&quot; #we want the first column and then columns 9-55 beag_allIND_final&lt;-beag_allIND[,c(1,9:55)] #now we have a dataframe with 31905 rows and 48 columns #let&#39;s write a table that we can use to run in pcangsd write.table(beag_allIND_final, &quot;geno_like_filt_allIND.beagle&quot;, sep=&quot;\\t&quot;, row.names = F) #by default write.table outputs a dataframe with row names we dont want that ### This writes a file to our Marine Genomics data directory. # pcangsd needs the file to be gzipped $ gzip geno_like_filt_allIND.beagle # and now we can run our pcangsd code $ python3 ../../pcangsd/pcangsd.py -beagle geno_like_filt_allIND.beagle.gz -o pca_out_allind -threads 28 and then we can go back to R and rerun our code to calculate the eigenvalues and eigenvectors for this covariance matrix. #read in the new covariance matrix cov_allind&lt;-as.matrix(read.table(&quot;pca_out_allind.cov&quot;)) #&gt; Warning in file(file, &quot;rt&quot;): cannot open file &#39;pca_out_allind.cov&#39;: No such file or directory #&gt; Error in file(file, &quot;rt&quot;): cannot open the connection #calculate eigenvalues e_allind&lt;-eigen(cov_allind) #&gt; Error in as.matrix(x): object &#39;cov_allind&#39; not found #plot the data plot(e_allind$vectors[,1:2]) #&gt; Error in h(simpleError(msg, call)): error in evaluating the argument &#39;x&#39; in selecting a method for function &#39;plot&#39;: object &#39;e_allind&#39; not found #how much variance does our pc explain e_allind$values/sum(e_allind$values) #&gt; Error in eval(expr, envir, enclos): object &#39;e_allind&#39; not found "],["week-9--fst-and-outlier-analysis.html", "10 Week 9- Fst and outlier analysis 10.1 Download the data 10.2 Getting R set up 10.3 Finding outliers using pcadapt 10.4 pcadapt Exercises Practice Questions 10.5 Using Fst to find outliers 10.6 OutFLANK Practice Practice Questions", " 10 Week 9- Fst and outlier analysis The lecture for this week focused on *Fst** and outlier analysis can be found here. Last week we used PCA to take a broad look at whether populations were genetically distinct. This week we learn how to identify particular SNPs that are driving patterns of divergence between populations. These SNPs could represent signals of selection. We will import the data and take a first look in bash and then we will do analysis and generate the plots in RStudio. 10.1 Download the data We first need to download the data. Use the link below to download it to jetstream, and then us the tar command to un-compress it. wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week9_semester.tar.gz tar -xzvf week9_semester.tar.gz Again this week we are using the data from the Xuereb et al. paper on P. californicus. Our data consists of a SNP file in VCF format and a metadata file which tells you which site each sample came from. Lets navigate into the Week 9 directory and take a look at the vcf file: cd MarineGenomics/Week9 head filtered_3699snps_californicus.vcf 10.2 Getting R set up Now move off of Terminal and open R Studio. The first thing we need to do is download a few different packages. You only need to do this the first time you use each package on your machine. Today we will be using two different packages: pcadapt and OutFLANK and both of them have some other packages they need (dependencies). While many packages can be downloaded in R using the install.packages function, some cannot because they are stored in specialty repositories. You can see that pcadapt can be downloaded from CRAN using the standard command, but OutFLANK and qvalue need to be downloaded from other sources. We likely wont have time to do both pcadapt and outflank today, so well leave outflank for your benefit if youre interested in running through it on your own. To save all of the progress we make today, first create your own script in the Week9 folder for this week named Week9Script.R If you have an error reading Warning in install.packages : package ?BiocManager? is not available for this version of R Go to Help, then Check for Updates and update your R version. You also may need to restart your R. After you have successfully downloaded all the packages needed for today, go ahead and get a new R script started. Remember to do lots of commenting with # so it is easy to come back to your code later! 10.3 Finding outliers using pcadapt Today we will use two different methods to find outliers. The first is based on PCA, which you learned about last week. For this you will use the pcadapt package. We will just use a few functions, but you can do a lot with this package; the documentation is found here I always like to call the different R packages I plan to use at the very top of the script so I just have to call them once: library(pcadapt) library(qvalue) Lets start by setting a working directory. Im going to set it to where we downloaded the data for week 9. I also like to define the paths to my input files at the top of the script, that way if I want to use a slightly different input file or if I want to share my script its easy to see how to change it. vcf.path=&quot;filtered_3699snps_californicus.vcf&quot; meta.path=&quot;californicus_metadata.csv&quot; Lets read in the genetic data from the VCF file. Because VCF is a common file format for SNP data, pcadapt comes with a special function for reading this type of file. You will get a warning message if you run on your own computer, but dont worry about it. #&gt; No variant got discarded. #&gt; Summary: #&gt; #&gt; - input file: filtered_3699snps_californicus.vcf #&gt; - output file: C:\\Users\\SAPCaps\\AppData\\Local\\Temp\\RtmpOa9kcU\\filefb2c4e7a1330.pcadapt #&gt; #&gt; - number of individuals detected: 717 #&gt; - number of loci detected: 3699 #&gt; #&gt; 3699 lines detected. #&gt; 717 columns detected. Now lets read in the metadata. This is a CSV (comma separated values) file, which is a spreadsheet-style format that can be output from programs like Excel. The columns are separated with commas. Once you load the file, take a minute to examine the contents. meta &lt;- read.csv(meta.path) head(meta) #&gt; ID SITE LAT LONG Group #&gt; 1 CRA_1756 CRA 50.5212 -126.5646 South #&gt; 2 CRA_1757 CRA 50.5212 -126.5646 South #&gt; 3 CRA_1758 CRA 50.5212 -126.5646 South #&gt; 4 CRA_1759 CRA 50.5212 -126.5646 South #&gt; 5 CRA_1760 CRA 50.5212 -126.5646 South #&gt; 6 CRA_1761 CRA 50.5212 -126.5646 South IMPORTANT NOTE: Here the samples in the VCF are in the same order as the samples in the metadata file. Triple check ahead of time that this is the case otherwise you may get some weird results! Now that weve read in all the data, the first step for pcadapt is to make a PCA. Last week you used PCAngsd to do this, but there are many other packages that work as well. Heres how we do it in pcadapt: x &lt;- pcadapt(input=genos,K=5) plot(x,option=&quot;screeplot&quot;) We can see from this plot that the vast majority of genetic variation can be explained by the first two principal components axes. Now we can plot the actual PCA. Here I will color by the Group. plot(x,option=&quot;scores&quot;,pop=meta$Group) You can see that there is separation between the North and South Groups, but it is not perfect. Now we want to see which of the &gt;3000 SNPs is driving the variation we see on the PCA plot(x,option=&quot;manhattan&quot;) This gives us a visual idea of which SNPs might be associated with population differences. If we want to identify statistical outliers, we first need to adjust the p-values: qval &lt;- qvalue(x$pvalues)$qvalues outliers &lt;- which(qval&lt;0.1) length(outliers) #&gt; [1] 143 10.4 pcadapt Exercises Practice Questions Perhaps we are particularly interested in SNPs associated with latitude. A first step might be to ask whether either of our PC axes represent latitudinal variation. x$scores gives you the PC loadings for each sample along the first and second PC axes. Combine these with the metadata to see whether either PC axes are correlated with latitude (for our purposes you can just visualize the relationship, you dont have to run the statistics.) Solution If we are only interested in SNPs associated with a single principal component, we can get pcadapt to give us correlation scores for each PC axis independently. Using the guidance provided in the documentation here make a manhattan plot of SNPs driving variation along the first principal component only Solution 10.5 Using Fst to find outliers Another way to look for signals of selection is to use Fst. Fst is a measure of genetic differentiation between populations. When we use Fst to test for signals of selection, we ask if any SNPs are more divergent than expected given the genome-wide differentiation. Lets start by loading a couple libraries: library(OutFLANK) #&gt; Error in library(OutFLANK): there is no package called &#39;OutFLANK&#39; library(vcfR) We use the vcfR library to load the vcf file and extract just the genotypes. Remember there is a lot of information in VCF files, probably more than we want. vcfR has many different functions for extracting just some information so that you can use it any way you want data &lt;- read.vcfR(vcf.path) #&gt; Scanning file to determine attributes. #&gt; File attributes: #&gt; meta lines: 9 #&gt; header_line: 10 #&gt; variant count: 3699 #&gt; column count: 726 #&gt; Meta line 9 read in. #&gt; All meta lines processed. #&gt; gt matrix initialized. #&gt; Character matrix gt created. #&gt; Character matrix gt rows: 3699 #&gt; Character matrix gt cols: 726 #&gt; skip: 0 #&gt; nrows: 3699 #&gt; row_num: 0 #&gt; Processed variant 1000 Processed variant 2000 Processed variant 3000 Processed variant: 3699 #&gt; All variants processed geno &lt;- extract.gt(data) dim(geno) #&gt; [1] 3699 717 head(geno[,1:10]) #&gt; CRA_1756 CRA_1757 CRA_1758 CRA_1759 CRA_1760 CRA_1761 CRA_1762 CRA_1763 CRA_1764 CRA_1765 #&gt; 7_11 &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; #&gt; 12_31 &quot;0/1&quot; &quot;1/1&quot; NA &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; &quot;0/1&quot; &quot;0/0&quot; #&gt; 10_7 &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; #&gt; 10_18 &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/1&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; #&gt; 10_32 &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; #&gt; 101_21 &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; NA &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; &quot;0/0&quot; Check out the OutFLANK manual here. Notice that as our genotypes look like 0/0, 0/1, and 1/1. But OutFLANK wants them to be 0, 1, or 2. The code below fixes this problem: G &lt;- geno #we are doing this because we will be running a lot of different things with G, and if we mess up we want to be able to go back to geno G[geno %in% c(&quot;0/0&quot;)] &lt;- 0 G[geno %in% c(&quot;0/1&quot;)] &lt;- 1 G[geno %in% c(&quot;1/1&quot;)] &lt;- 2 G[is.na(G)] &lt;- 9 tG &lt;- t(G) dim(tG) #&gt; [1] 717 3699 Now tG should be in the input format OutFLANK needs, with SNPs as columns and individuals as rows. Now we can calculate Fst for each SNP. For this analysis we are just going to calculate Fst between the northernmost and the southernmost populations. This is a good time to practice our subsetting skills! For the second command: we are only taking the rows of tG that are true, or only taking the rows of tG that have one of these two populations TBL or AK4 In the third command we are just using a slightly different method for subsetting with the command subset. If we subset meta we should end up with 62 rows (SNPs) and 2 columns (populations). subpops &lt;- c(&quot;TBL&quot;,&quot;AK4&quot;) subgen &lt;- tG[meta$SITE%in%subpops,] #subset method 1 submeta &lt;- subset(meta,SITE%in%subpops) #subset method 2 Now we should check if these two vectors are identical: identical(rownames(subgen),as.character(submeta$ID)) #&gt; [1] TRUE Since our genotype matrix and our metadata matrix are in the same order, we can combine them. Now we can calculate Fst between these two populations: locusNames= names our loci 1,2,3 etc popNames= names our populations with the SITE labels fst &lt;- MakeDiploidFSTMat(subgen,locusNames=1:ncol(subgen),popNames=submeta$SITE) #&gt; Error in MakeDiploidFSTMat(subgen, locusNames = 1:ncol(subgen), popNames = submeta$SITE): could not find function &quot;MakeDiploidFSTMat&quot; head(fst) #&gt; Error in h(simpleError(msg, call)): error in evaluating the argument &#39;x&#39; in selecting a method for function &#39;head&#39;: object &#39;fst&#39; not found hist(fst$FST,breaks=50) #&gt; Error in hist(fst$FST, breaks = 50): object &#39;fst&#39; not found summary(fst$FST) #highest FST is higher than the mean (which is a good sign) #&gt; Error in summary(fst$FST): object &#39;fst&#39; not found Reminder: He= heterozygosity FST= measure of differentiation Once weve calculated Fst between the two populations for each SNP individually, we want to determine whether some SNPs are statistical outliers - that is, more differentiated than we would expect. OutFLANK does this by fitting a Chi-Squared distribution to the data and looking to see if the tails of the Chi-Squared distribution have more SNPs than expected: OF &lt;- OutFLANK(fst,LeftTrimFraction=0.01,RightTrimFraction=0.01, Hmin=0.05,NumberOfSamples=2,qthreshold=0.01) #&gt; Error in OutFLANK(fst, LeftTrimFraction = 0.01, RightTrimFraction = 0.01, : could not find function &quot;OutFLANK&quot; OutFLANKResultsPlotter(OF,withOutliers=T, NoCorr=T,Hmin=0.1,binwidth=0.005, Zoom=F,RightZoomFraction=0.05,titletext=NULL) #&gt; Error in OutFLANKResultsPlotter(OF, withOutliers = T, NoCorr = T, Hmin = 0.1, : could not find function &quot;OutFLANKResultsPlotter&quot; FSTbar=mean FST across the whole genome The yellow bars are the histogram of the FST values The blue line is the Chi-Squared Distribution fit to the data It is hard to fit the Chi-Squared Distribution to a population with high gene flow, aka this method doesnt work great for marine species We will just progress forward, but keep this in mind for any future work you may do. Its a little hard to tell from these plots, but there may be some SNPs with high Fst even where the distribution predicts there should be none. To find these SNPs, we ask which SNPs are statistical outliers? P1 &lt;- pOutlierFinderChiSqNoCorr(fst,Fstbar=OF$FSTNoCorrbar, dfInferred=OF$dfInferred,qthreshold=0.05,Hmin=0.1) #&gt; Error in pOutlierFinderChiSqNoCorr(fst, Fstbar = OF$FSTNoCorrbar, dfInferred = OF$dfInferred, : could not find function &quot;pOutlierFinderChiSqNoCorr&quot; outliers &lt;- P1$OutlierFlag==TRUE #which of the SNPs are outliers? #&gt; Error in eval(expr, envir, enclos): object &#39;P1&#39; not found table(outliers) #&gt; outliers #&gt; 20 96 129 130 139 179 195 196 197 198 242 282 283 294 296 347 370 410 418 488 511 562 605 630 729 737 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 779 780 789 888 889 891 901 908 950 968 969 984 997 999 1000 1005 1033 1081 1087 1094 1141 1144 1286 1295 1330 1361 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 1382 1397 1410 1447 1463 1464 1539 1540 1562 1564 1572 1640 1663 1679 1710 1711 1747 1755 1783 1784 1807 1808 1827 1830 1922 1941 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 1943 1963 1978 2033 2048 2052 2162 2174 2181 2184 2199 2225 2238 2290 2291 2292 2294 2307 2329 2330 2333 2371 2382 2485 2487 2488 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 2522 2746 2747 2773 2785 2789 2790 2813 2828 2934 2959 2960 2963 2985 2992 2993 3007 3035 3055 3072 3087 3137 3159 3225 3290 3306 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 3309 3344 3345 3360 3372 3373 3404 3412 3514 3538 3620 3637 3671 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 This doesnt add up to 3699 so some of the SNPs were not even tested. This is okay because there is an internal program that doesnt test any SNPs with extremely low heterozygosity. Looks like there are 17 outlier SNPs. Now we can make a manhattan plot! We can even plot the outliers in a different color: plot(P1$LocusName,P1$FST,xlab=&quot;Position&quot;,ylab=&quot;FST&quot;,col=rgb(0,0,0,alpha=0.1)) #&gt; Error in h(simpleError(msg, call)): error in evaluating the argument &#39;x&#39; in selecting a method for function &#39;plot&#39;: object &#39;P1&#39; not found points(P1$LocusName[outliers],P1$FST[outliers],col=&quot;magenta&quot;) #&gt; Error in points(P1$LocusName[outliers], P1$FST[outliers], col = &quot;magenta&quot;): object &#39;P1&#39; not found 10.6 OutFLANK Practice Practice Questions We tested for Fst outliers between just the northernmost and southernmost sampling sites. Do we get more outliers that we expect between any two sites? Choose two sites within the northern and southern groups and see if there are any outliers. Solution subpops &lt;- c(&quot;AK3&quot;,&quot;AK4&quot;) subgen &lt;- tG[meta$SITE%in%subpops,] submeta &lt;- subset(meta,SITE%in%subpops) identical(rownames(subgen),as.character(submeta$ID)) #&gt; [1] TRUE fst &lt;- MakeDiploidFSTMat(subgen,locusNames=1:ncol(subgen),popNames=submeta$SITE) #&gt; Error in MakeDiploidFSTMat(subgen, locusNames = 1:ncol(subgen), popNames = submeta$SITE): could not find function &quot;MakeDiploidFSTMat&quot; OF &lt;- OutFLANK(fst,LeftTrimFraction=0.01,RightTrimFraction=0.01, Hmin=0.05,NumberOfSamples=2,qthreshold=0.01) #&gt; Error in OutFLANK(fst, LeftTrimFraction = 0.01, RightTrimFraction = 0.01, : could not find function &quot;OutFLANK&quot; P1 &lt;- pOutlierFinderChiSqNoCorr(fst,Fstbar=OF$FSTNoCorrbar, dfInferred=OF$dfInferred,qthreshold=0.05,Hmin=0.1) #&gt; Error in pOutlierFinderChiSqNoCorr(fst, Fstbar = OF$FSTNoCorrbar, dfInferred = OF$dfInferred, : could not find function &quot;pOutlierFinderChiSqNoCorr&quot; outliers &lt;- P1$OutlierFlag==TRUE #&gt; Error in eval(expr, envir, enclos): object &#39;P1&#39; not found table(outliers) #&gt; outliers #&gt; 20 96 129 130 139 179 195 196 197 198 242 282 283 294 296 347 370 410 418 488 511 562 605 630 729 737 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 779 780 789 888 889 891 901 908 950 968 969 984 997 999 1000 1005 1033 1081 1087 1094 1141 1144 1286 1295 1330 1361 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 1382 1397 1410 1447 1463 1464 1539 1540 1562 1564 1572 1640 1663 1679 1710 1711 1747 1755 1783 1784 1807 1808 1827 1830 1922 1941 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 1943 1963 1978 2033 2048 2052 2162 2174 2181 2184 2199 2225 2238 2290 2291 2292 2294 2307 2329 2330 2333 2371 2382 2485 2487 2488 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 2522 2746 2747 2773 2785 2789 2790 2813 2828 2934 2959 2960 2963 2985 2992 2993 3007 3035 3055 3072 3087 3137 3159 3225 3290 3306 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; 3309 3344 3345 3360 3372 3373 3404 3412 3514 3538 3620 3637 3671 #&gt; 1 1 1 1 1 1 1 1 1 1 1 1 1 #FALSE TRUE # 1284 1 "],["week-10-genome-wide-association-study-gwas.html", "11 Week 10: Genome wide association study (GWAS) 11.1 load in the data 11.2 install packages in R 11.3 install angsd and pcangsd (again?) 11.4 The data 11.5 take Beagle file and generate lrt file 11.6 take lrt file and make a manhattan plot 11.7 Exercises", " 11 Week 10: Genome wide association study (GWAS) This week were going to show you how to perform a genome wide association study or GWAS. The lecture for this week can be found here 11.1 load in the data The data for this week consist of several files in beagle format, a genome index file (.fai), and a few small text files. wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/week10_semester.tar.gz tar -xzvf week10_semester.tar.gz #and one more file that was too big to upload with the others cd MarineGenomicsData/week10 wget https://raw.githubusercontent.com/BayLab/MarineGenomicsData/main/salmon_chr6_19ind.BEAGLE.PL.gz 11.2 install packages in R Well be using the package qqman to make a manhattan plot. #install.package(&quot;qqman&quot;) 11.3 install angsd and pcangsd (again?) If you missed one of the previous weeks and didnt get pcangsd or angsd installed. Or if you had to relaunch your jetstream instance, you will need to reinstall them. Here are the scripts to install them. They take a few minutes to install. There is no need to install these again if you installed them last time. You can first check if they are installed with: angsd -h 11.3.1 install angsd cd git clone --recursive https://github.com/samtools/htslib.git git clone https://github.com/ANGSD/angsd.git cd htslib;make;cd ../angsd ;make HTSSRC=../htslib cd 11.3.2 for pcangsd install pip for python curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python3 get-pip.py we need to add our home directory to the path for pip look at path echo $PATH add the location of pip to our path export PATH=&quot;$HOME/.local/bin:$PATH&quot; then install pcangsd git clone https://github.com/Rosemeis/pcangsd.git cd pcangsd/ pip install --user -r requirements.txt #this installs additional requirements for pcangsd python3 setup.py build_ext --inplace 11.4 The data The data were working on for the GWAS comes from the paper by Kijas et al. 2018 where they studied the genetic basis of sex determination in the atlantic Salmon. The paper can be found here. In short they examined the genetic basis of sex in 19 salmon for which they have whole genome sequence data. Well only be looking at two chromosomes (2 and 6) of this data. 11.5 take Beagle file and generate lrt file To do the test of genome wide association we need to take our Beagle file and test whether there is an association with our phenotype (in this case whether a fish has a male or female phenotype). The phenotypes are coded as 0 = Female and 1 = Male in the phenobin file. $HOME/angsd/angsd -doMaf 4 -beagle salmon_chr2_19ind.BEAGLE.PL.gz -yBin phenobin -doAsso 2 -fai Salmon.fai This will generate several output files labeled angsdput. Well use the file with the lrt0 extension to plot our manattan plot. 11.6 take lrt file and make a manhattan plot This script reads in the data and then removes values that cant be used in the manhattan plot. #read in the data lrt&lt;-read.table(gzfile(&quot;angsdput.lrt0.gz&quot;), header=T, sep=&quot;\\t&quot;) #look at it str(lrt) #&gt; &#39;data.frame&#39;: 224460 obs. of 8 variables: #&gt; $ Chromosome : int 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ Position : int 13 59 74 153 548 550 613 1182 1361 1533 ... #&gt; $ Major : chr &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;C&quot; ... #&gt; $ Minor : chr &quot;T&quot; &quot;A&quot; &quot;A&quot; &quot;G&quot; ... #&gt; $ Frequency : num 0.237 0.5 0.316 0.184 0.316 ... #&gt; $ N : int 19 19 19 19 19 19 19 19 19 19 ... #&gt; $ LRT : num -999 -999 -999 -999 -999 -999 -999 -999 -999 -999 ... #&gt; $ high_WT.HE.HO: chr &quot;10/9/0&quot; &quot;1/17/1&quot; &quot;7/12/0&quot; &quot;12/7/0&quot; ... #we have a few LRT values that are -999, we should remove them. How many do we have? length(which(lrt$LRT == -999)) #&gt; [1] 210597 #210597 #most are filtered out length(lrt$LRT) #&gt; [1] 224460 #224460 length(lrt$LRT)-length(which(lrt$LRT == -999)) #&gt; [1] 13863 #[1] 13863 #remove the values that are not -999 and that are negative lrt_filt&lt;-lrt[-c(which(lrt$LRT == -999),which(lrt$LRT &lt;= 0)),] hist(lrt_filt$LRT, breaks=50) Everything looks good to proceed to making our manhattan plot. the function manhattan requires each SNP to have its own name. Lets make a vector for rownumbers that start with the letter r. We also need to convert our LRT values to pvalues. Well use the command dchisq to get pvalues. manhattan(lrt_filt, chr=&quot;Chromosome&quot;, bp=&quot;Position&quot;, p=&quot;pvalue&quot;) #&gt; Error in manhattan(lrt_filt, chr = &quot;Chromosome&quot;, bp = &quot;Position&quot;, p = &quot;pvalue&quot;): could not find function &quot;manhattan&quot; Lets look at a qq-plot of our pvalues to check the model fit qqnorm(lrt_filt$pvalue) This looks a bit weird, we would expect it to be mostly a straight line with some deviations at the upper right. If we were moving forward with this analyses wed want to do more filtering of our data. We can highlight the values that are exceed a threshold. There are several ways to determine a threshold, but one is to make a vector of random phenotypes and re-run our association test. We can then set the highest LRT value from the random phenotype test as our upper limit for our plot with the actual phenotypes. Its important that when we write the rando_pheno file that we dont have row names or column names, the default in R. If you have these youll like have an error that the number of individuals doesnt match the number of phenotypes when you run angsd. #make a vector with 19 1&#39;s and 0&#39;s x&lt;-sample(c(1,0), 19, replace=T) #write this to our week 8 directory write.table(x, &quot;rando_pheno&quot;, row.names = F, col.names = F) And now use that phenotype file to run our association test again, making sure to specify a different output file. $HOME/angsd/angsd -doMaf 4 -beagle salmon_chr2_19ind.BEAGLE.PL.gz -yBin rando_pheno -doAsso 2 -fai Salmon.fai -out randotest And rerun the code in R to see what our maximum LRT values are in this random phenotype test. lrt_rando&lt;-read.table(gzfile(&quot;randotest.lrt0.gz&quot;), header=T, sep=&quot;\\t&quot;) #we need to remove those -999 values again rando_filt&lt;-lrt_rando[-c(which(lrt_rando$LRT == -999),which(lrt_rando$LRT &lt;= 0)),] summary(rando_filt$LRT, na.rm=T) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 0.000001 0.214816 0.669551 Inf 2.029913 Inf 1 #we have some Inf values we also need to remove, let&#39;s add those to our filtering line above rando_filt&lt;-lrt_rando[-c(which(lrt_rando$LRT == -999),which(lrt_rando$LRT &lt;= 0), which(lrt_rando$LRT == Inf)),] max(rando_filt$LRT, na.rm=T) #&gt; [1] 12.33287 So we can highlight all of the SNPs that have an LRT greater than 12 in our association test. #make a list of the candidates candidates&lt;-lrt_filt[which(lrt_filt$LRT &gt; 36),]$SNP #refer to that list of candidates with the highlight parameter manhattan(lrt_filt, chr=&quot;Chromosome&quot;, bp=&quot;Position&quot;, p=&quot;pvalue&quot;, highlight = candidates) #&gt; Error in manhattan(lrt_filt, chr = &quot;Chromosome&quot;, bp = &quot;Position&quot;, p = &quot;pvalue&quot;, : could not find function &quot;manhattan&quot; Comparing our results to the Kijas et al. 2018 paper, we have a similar pattern of many SNPs across the chromosome showing a relationship with phenotype (sex). Interestingly this paper found that there are three chromosomes that are associated with sex in Atlantic Salmon, but not all chromosomes give a strong signal in all individuals. For example, only three male individuals were found to have a clear association with chromosome 2, and the other males in the study were found to have an association with chromosomes 3 and 6. These results highlight the fluid nature of sex determination in animals, even those with a genetic basis to sex determination. For the exercise youll take a closer look at chromosome 6, where youll try to find the individuals that are male from a PCA plot. 11.7 Exercises 1.For this exercise, we want to see which individuals actually show genomic levels of variation at chromosome 6. Using the code that we ran for week 6. Make a PCA plot of the salmon_chr3_19ind.BEAGLE.PL.gz file. Use the function indentify() to find the points that are clustering apart from the other points. Verify that these are males by referring to the table in the Kijas et al. paper here. Note their individuals are in the same order as our samples though the names dont match. Solution #run pcangsd on our chr6 data python3 ../../pcangsd/pcangsd.py -beagle salmon_chr6_19ind.BEAGLE.PL.gz -o pca6_out -threads 28 In R make a PCA plot #read in the data cov&lt;-as.matrix(read.table(&quot;pca6_out.cov&quot;)) #compute the eigen values e&lt;-eigen(cov) #how much variation are we explaining here? e$values/sum(e$values) #&gt; [1] 0.1379496011 0.1246126748 0.0969103029 0.0920252715 0.0774570788 0.0685079248 0.0605808079 0.0554445095 0.0493884028 #&gt; [10] 0.0444119246 0.0423030396 0.0368221717 0.0329494215 0.0316195242 0.0269138675 0.0184940705 0.0149661517 -0.0006450986 #&gt; [19] -0.0107116466 #looks like 13.7% is explained by the first axis #make a plot of the first two axes plot(e$vectors[,1:2]) #use identify to find the points that are clustered apart from the others #identify(e$vectors[,1:2]) This doesn&#39;t work in R studio so we&#39;ll show the plot below #If you select the points on the left most of the screen, you will find they are rows # 1, 2, and 12 #If you select the other two points that are leftmost from the remaining points you also get # 16 and 19 Looking at the table in the Kilas paper, the Animals in rows 1, 2, and 12 are Male, but its rows 17 and 18 that are next not 16 and 19, so we may have somethings out of order in our list. Make a manhattan plot for chromosome 6. You will need to make a new phenotype file that has 1s for all the males identified in problem 1. Alterntatively, you can make a phenotype file from table 1 in Kilas et al. 2018, setting the animals that were found to be male on chr 6 to 1 and all other animals to 0. Solution $ gzip geno_like_filt_allIND.beagle and now we can run our pcangsd code #make a phenotype file, note you can also do this in nano or vim in the terminal. Whichever makes sense for you. pheno_chr6&lt;-c(1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0) write.table(pheno_chr6, file = &quot;phenobin_ch6&quot; ) #then in the terminal run our command in angsd again using the chr6 BEAGLE file and the phenotype file we just made # $HOME/angsd/angsd -doMaf 4 -beagle salmon_chr6_19ind.BEAGLE.PL.gz -yBin phenobin_ch6 -doAsso 2 -fai Salmon.fai -out chr6_out #and then we can remake our manhattan plot by copying and pasting the code above lrt&lt;-read.table(gzfile(&quot;chr6_out.lrt0.gz&quot;), header=T, sep=&quot;\\t&quot;) #&gt; Warning in open.connection(file, &quot;rt&quot;): cannot open compressed file &#39;chr6_out.lrt0.gz&#39;, probable reason &#39;No such file or directory&#39; #&gt; Error in open.connection(file, &quot;rt&quot;): cannot open the connection #look at it str(lrt) #&gt; &#39;data.frame&#39;: 224460 obs. of 8 variables: #&gt; $ Chromosome : int 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ Position : int 13 59 74 153 548 550 613 1182 1361 1533 ... #&gt; $ Major : chr &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;C&quot; ... #&gt; $ Minor : chr &quot;T&quot; &quot;A&quot; &quot;A&quot; &quot;G&quot; ... #&gt; $ Frequency : num 0.237 0.5 0.316 0.184 0.316 ... #&gt; $ N : int 19 19 19 19 19 19 19 19 19 19 ... #&gt; $ LRT : num -999 -999 -999 -999 -999 -999 -999 -999 -999 -999 ... #&gt; $ high_WT.HE.HO: chr &quot;10/9/0&quot; &quot;1/17/1&quot; &quot;7/12/0&quot; &quot;12/7/0&quot; ... #remove the values that are not -999 and that are negative lrt_filt&lt;-lrt[-c(which(lrt$LRT == -999),which(lrt$LRT &lt;= 0)),] lrt_filt$SNP&lt;-paste(&quot;r&quot;,1:length(lrt_filt$Chromosome), sep=&quot;&quot;) #we also need to make sure we don&#39;t have any tricky values like those below lrt_filt&lt;-lrt_filt[-c(which(lrt_filt$pvalue == &quot;NaN&quot; ), which(lrt_filt$pvalue == &quot;Inf&quot;), which(lrt_filt$LRT == &quot;Inf&quot;)),] #get pvalues lrt_filt$pvalue&lt;-dchisq(lrt_filt$LRT, df=1) #make our plot manhattan(lrt_filt, chr=&quot;Chromosome&quot;, bp=&quot;Position&quot;, p=&quot;pvalue&quot;) #&gt; Error in manhattan(lrt_filt, chr = &quot;Chromosome&quot;, bp = &quot;Position&quot;, p = &quot;pvalue&quot;): could not find function &quot;manhattan&quot; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
